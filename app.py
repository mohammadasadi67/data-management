import streamlit as st
import pandas as pd
import plotly.express as px
from io import BytesIO
from supabase import create_client, Client
import base64
from datetime import datetime, timedelta, time as datetime_time
import re
import time
import numpy as np
import json # Added for better handling of Supabase data

# --- Supabase Configuration ---
SUPABASE_URL = "https://rlutsxvghmhrgcnqbmch.supabase.co"
SUPABASE_KEY = "eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJzdXBhYmFzZSIsInJlZiI6InJsdXRzeHZnaG1ocmdjbnFibWNoIiwicm9sZSI6InNlcnZpY2Vfcm9sZSIsImlhdCI6MTc0NTEyODk5MSwiZXhwIjoyMDYwNzA0OTkxfQ.VPxJbrPUw4E-MyRGklQMcxveUTznNlWLhPO-mqrHv9c"

# Initialize Supabase client globally
supabase: Client = create_client(SUPABASE_URL, SUPABASE_KEY)

# --- Password for Archive Deletion ---
ARCHIVE_DELETE_PASSWORD = "beautifulmind"

# --- Page Configuration and State Initialization ---
st.set_page_config(layout="wide", page_title="OEE Analysis Dashboard", page_icon="ğŸ­")

if 'page' not in st.session_state:
    st.session_state.page = "Dashboard"
if 'prod_df' not in st.session_state:
    st.session_state.prod_df = pd.DataFrame()
if 'err_df' not in st.session_state:
    st.session_state.err_df = pd.DataFrame()
if 'archived_files' not in st.session_state:
    st.session_state.archived_files = []
if 'prod_files_loaded' not in st.session_state:
    st.session_state.prod_files_loaded = False
if 'err_files_loaded' not in st.session_state:
    st.session_state.err_files_loaded = False

# --- Helper Functions (Reconstructed based on context) ---

def parse_filename_date_to_datetime(filename):
    """Extracts ddmmyyyy from filename and converts it to a datetime.date object."""
    date_match = re.search(r'(\d{8})', filename)
    if date_match:
        try:
            return datetime.strptime(date_match.group(1), '%d%m%Y').date()
        except ValueError:
            pass
    return None

def load_data_from_supabase(table_name):
    """Loads all data from a specified Supabase table."""
    try:
        response = supabase.table(table_name).select("*").execute()
        data = response.data
        return pd.DataFrame(data)
    except Exception as e:
        st.error(f"Error loading data from Supabase table {table_name}: {e}")
        return pd.DataFrame()

def list_archived_files():
    """Lists files from the 'archive' Supabase storage bucket."""
    try:
        # Use the global supabase client to access Storage
        response = supabase.storage.from_("archive").list()
        
        # Supabase list() returns a list of file metadata dictionaries.
        if isinstance(response, list):
            # Filter out the directory listing metadata object (name: '.')
            files = [f['name'] for f in response if f['name'] != '.']
            return files
        else:
            # Handle potential error dictionaries returned by the API call
            error_message = response.get('message', 'Ø®Ø·Ø§ÛŒ Ù†Ø§Ù…Ø´Ø®Øµ') if isinstance(response, dict) else 'ÙØ±Ù…Øª Ù¾Ø§Ø³Ø® Ù†Ø§Ù…Ø¹ØªØ¨Ø±'
            st.error(f"Ø®Ø·Ø§ Ø¯Ø± Ù„ÛŒØ³Øª Ú©Ø±Ø¯Ù† ÙØ§ÛŒÙ„â€ŒÙ‡Ø§ÛŒ Ø¢Ø±Ø´ÛŒÙˆ: {error_message}")
            return []
            
    except Exception as e:
        st.error(f"Ø®Ø·Ø§ÛŒ Ú©Ù„ÛŒ Ø¯Ø± Ø¯Ø³ØªØ±Ø³ÛŒ Ø¨Ù‡ Ø¢Ø±Ø´ÛŒÙˆ Supabase: {e}")
        return []

def process_uploaded_files(uploaded_files, file_type):
    """Processes uploaded Excel files for production or error data."""
    all_data = []
    
    # Define columns expected for each type
    expected_cols = {
        'production': ['MachineType', 'Product', 'ProductionTypeForTon', 'StartTime', 'StopTime', 'TotalGood', 'IdealTime', 'AvailableTime'],
        'error': ['MachineType', 'Error', 'Duration', 'StartTime', 'StopTime', 'Date']
    }
    
    for file in uploaded_files:
        try:
            df = pd.read_excel(file)
            
            # Standardize column names (example logic)
            df.columns = df.columns.str.strip().str.replace(' ', '')
            
            # Check for necessary columns
            if not all(col in df.columns for col in expected_cols[file_type]):
                st.warning(f"File {file.name} is missing expected columns for {file_type} data. Skipping.")
                continue

            # Add source info
            df['SourceFile'] = file.name
            df['Date'] = parse_filename_date_to_datetime(file.name)
            
            # Basic type conversion/cleaning
            if file_type == 'production':
                df['StartTime'] = pd.to_datetime(df['StartTime'], errors='coerce')
                df['StopTime'] = pd.to_datetime(df['StopTime'], errors='coerce')
            elif file_type == 'error':
                df['StartTime'] = pd.to_datetime(df['StartTime'], errors='coerce')
                df['StopTime'] = pd.to_datetime(df['StopTime'], errors='coerce')
                # Ensure Duration is numeric
                df['Duration'] = pd.to_numeric(df['Duration'], errors='coerce').fillna(0)
                df['Error'] = df['Error'].astype(str) # Ensure error codes are strings for grouping

            all_data.append(df)
            
        except Exception as e:
            st.error(f"Error processing file {file.name}: {e}")
            
    if all_data:
        combined_df = pd.concat(all_data, ignore_index=True)
        return combined_df
    return pd.DataFrame()

def calculate_metrics(prod_df, err_df, group_cols):
    """
    Calculates key OEE metrics (Availability, Performance, Quality, OE, Line Efficiency) 
    grouped by specified columns.
    """
    if prod_df.empty:
        return pd.DataFrame()

    # 1. Total Planned Production Time (PPT) and Available Time (AT)
    # Assumed logic: AT is given directly in the production data
    
    # 2. Total Ideal Time (IT) and Total Good Product Count (Good_Qty)
    prod_metrics = prod_df.groupby(group_cols).agg(
        Total_Available_Time=('AvailableTime', 'sum'), # T_A (Total Available)
        Total_Ideal_Time=('IdealTime', 'sum'),         # T_I (Total Ideal Run Time)
        Total_Good_Qty=('TotalGood', 'sum'),           # Q_G (Good Quality Count)
        Total_Production_Tons=('ProductionTypeForTon', 'sum'), # Total Tons produced
        # Assuming we need to calculate Run Time (RT)
        Total_Actual_Time=('StartTime', lambda x: (prod_df.loc[x.index, 'StopTime'] - prod_df.loc[x.index, 'StartTime']).dt.total_seconds().sum() / 60) # Total Actual Run Time (minutes)
    ).reset_index()

    # Calculate Total Production Time (PT) and Down Time (DT)
    # This part requires assumptions about how DT is derived. 
    # Standard OEE: DT = AT - RT (or calculated from error logs)
    # Assuming AT is the max time machine was supposed to run (Total Available Time)
    
    prod_metrics['Total_Run_Time'] = prod_metrics['Total_Actual_Time'] # T_R (Actual Run Time) - Simplified

    # --- Metrics Calculation ---
    
    # 1. Availability (A) = Run Time / Available Time
    prod_metrics['Availability(%)'] = np.where(
        prod_metrics['Total_Available_Time'] > 0,
        (prod_metrics['Total_Run_Time'] / prod_metrics['Total_Available_Time']) * 100,
        0
    )

    # 2. Performance (P) = (Ideal Cycle Time * Total Quantity) / Run Time
    # Assuming Ideal Cycle Time is implicitly derived from (Total_Ideal_Time / Total_Good_Qty)
    # Let's use a simplified approach based on your available columns:
    # P = Ideal Time / Actual Run Time
    prod_metrics['Performance(%)'] = np.where(
        prod_metrics['Total_Run_Time'] > 0,
        (prod_metrics['Total_Ideal_Time'] / prod_metrics['Total_Run_Time']) * 100,
        0
    )
    
    # Cap Performance at 100% (or standard OEE definition)
    prod_metrics['Performance(%)'] = prod_metrics['Performance(%)'].clip(upper=100)
    
    # 3. Quality (Q) = Good Quantity / Total Quantity (Simplified)
    # Assuming Total Good Qty is the only measure available, we assume the denominator 
    # (Total Qty) is equal to TotalGood for simplicity or we need a specific rejection column.
    # We will use Quality = 100% for now unless reject data is available.
    prod_metrics['Quality(%)'] = 100 
    
    # 4. OE = A * P * Q (Since Q=100%, OE = A * P)
    prod_metrics['OE(%)'] = (prod_metrics['Availability(%)'] / 100) * (prod_metrics['Performance(%)'] / 100) * 100

    # 5. Line Efficiency (IE) = (Total Good Qty / Max Theoretical Qty) * 100
    # Assuming Max Theoretical Qty is implicitly related to Available Time and Ideal Time.
    # Line Efficiency (IE) = Ideal Run Time / Available Time
    prod_metrics['Line_Efficiency(%)'] = np.where(
        prod_metrics['Total_Available_Time'] > 0,
        (prod_metrics['Total_Ideal_Time'] / prod_metrics['Total_Available_Time']) * 100,
        0
    )
    
    # Cleanup and format for display
    for col in ['Availability(%)', 'Performance(%)', 'Quality(%)', 'OE(%)', 'Line_Efficiency(%)']:
        prod_metrics[col] = prod_metrics[col].round(2)

    return prod_metrics


# --- Navigation ---
def set_page(page_name):
    st.session_state.page = page_name

st.sidebar.title("ğŸ­ OEE Dashboard")
st.sidebar.button("ğŸ“Š Dashboard", on_click=set_page, args=("Dashboard",))
st.sidebar.button("ğŸ“ˆ Trend Analysis", on_click=set_page, args=("Trend Analysis",))
st.sidebar.button("ğŸ“‚ Data Management", on_click=set_page, args=("Data Management",))
st.sidebar.button("ğŸ“§ Contact Me", on_click=set_page, args=("Contact Me",))

# --- Main Application Logic ---

def Data_Analyzing_Dashboard():
    st.title("ğŸ“Š Ø¯Ø§Ø´Ø¨ÙˆØ±Ø¯ ØªØ­Ù„ÛŒÙ„ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ ØªÙˆÙ„ÛŒØ¯")
    st.markdown("---")

    if st.session_state.prod_df.empty:
        st.info("Ù„Ø·ÙØ§Ù‹ ÙØ§ÛŒÙ„â€ŒÙ‡Ø§ÛŒ ØªÙˆÙ„ÛŒØ¯ Ùˆ Ø®Ø·Ø§ÛŒ Ø®ÙˆØ¯ Ø±Ø§ Ø¯Ø± Ø¨Ø®Ø´ Data Management Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ Ú©Ù†ÛŒØ¯.")
        return

    final_prod_df = st.session_state.prod_df.copy()
    final_err_df = st.session_state.err_df.copy()
    
    # --- Filters ---
    with st.expander("ÙÛŒÙ„ØªØ±Ù‡Ø§ÛŒ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§", expanded=True):
        col1, col2, col3 = st.columns(3)
        
        # Determine available machine types
        all_machines = final_prod_df['MachineType'].unique().tolist() if 'MachineType' in final_prod_df.columns else []
        selected_machine = col1.selectbox("Ø§Ù†ØªØ®Ø§Ø¨ Ù…Ø§Ø´ÛŒÙ†:", ["All"] + all_machines)

        # Determine min/max dates
        min_date = final_prod_df['Date'].min() if not final_prod_df.empty and 'Date' in final_prod_df.columns else datetime.now().date()
        max_date = final_prod_df['Date'].max() if not final_prod_df.empty and 'Date' in final_prod_df.columns else datetime.now().date()

        date_range = col2.date_input("Ù…Ø­Ø¯ÙˆØ¯Ù‡ ØªØ§Ø±ÛŒØ®:", value=(min_date, max_date) if min_date != max_date else (min_date, min_date), min_value=min_date, max_value=max_date)

        # Apply Filters
        prod_df_filtered = final_prod_df.copy()
        err_df_filtered = final_err_df.copy()
        
        if selected_machine != "All":
            prod_df_filtered = prod_df_filtered[prod_df_filtered['MachineType'] == selected_machine].copy()
            if 'MachineType' in err_df_filtered.columns:
                err_df_filtered = err_df_filtered[err_df_filtered['MachineType'] == selected_machine].copy()
            
        if len(date_range) == 2:
            start_date, end_date = date_range[0], date_range[1]
            prod_df_filtered = prod_df_filtered[
                (prod_df_filtered['Date'] >= start_date) & 
                (prod_df_filtered['Date'] <= end_date)
            ].copy()
            
            if 'Date' in err_df_filtered.columns:
                 err_df_filtered = err_df_filtered[
                    (err_df_filtered['Date'] >= start_date) & 
                    (err_df_filtered['Date'] <= end_date)
                ].copy()

    # --- 1. Key Metrics Summary ---
    st.header("Ø®Ù„Ø§ØµÙ‡ Ú©Ù„ÛŒØ¯ Ù…ØªØºÛŒØ±Ù‡Ø§ÛŒ Ø¹Ù…Ù„Ú©Ø±Ø¯ (OEE)")
    
    # Calculate overall metrics
    overall_metrics = calculate_metrics(prod_df_filtered, err_df_filtered, group_cols=[])
    
    if not overall_metrics.empty:
        metric_row = overall_metrics.iloc[0]
        col_met1, col_met2, col_met3, col_met4, col_met5 = st.columns(5)
        
        col_met1.metric("Available Time (Min)", f"{metric_row['Total_Available_Time']:.0f}")
        col_met2.metric("OE (%)", f"{metric_row['OE(%)']:.2f}")
        col_met3.metric("Availability (%)", f"{metric_row['Availability(%)']:.2f}")
        col_met4.metric("Performance (%)", f"{metric_row['Performance(%)']:.2f}")
        col_met5.metric("Line Efficiency (IE) (%)", f"{metric_row['Line_Efficiency(%)']:.2f}")

    st.markdown("---")

    # --- 2. Combined Production Data Table ---
    # **Ø¯Ø±Ø®ÙˆØ§Ø³Øª Û²: Ø­Ø°Ù Ø³ØªÙˆÙ† Ø§ÙÛŒØ´Ù†Ø³ÛŒ Ø§Ø² Ø¬Ø¯ÙˆÙ„ Combined Production Data**
    st.subheader("Combined Production Data from Selected Files (Row-Level Efficiency)")
    df_display = prod_df_filtered.copy()
    
    # Ø­Ø°Ù Ø³ØªÙˆÙ† Efficiency(%) Ø¯Ø± ØµÙˆØ±Øª ÙˆØ¬ÙˆØ¯
    if 'Efficiency(%)' in df_display.columns:
        df_display = df_display.drop(columns=['Efficiency(%)'], errors='ignore')
        
    st.dataframe(df_display, use_container_width=True)

    st.markdown("---")

    # --- 3. Product Metrics Bar Chart (Replaced) ---
    # **Ø¯Ø±Ø®ÙˆØ§Ø³Øª Û³: Ø¬Ø§ÛŒÚ¯Ø²ÛŒÙ†ÛŒ Ù†Ù…ÙˆØ¯Ø§Ø± average efficiency by product Ø¨Ø§ Ø¨Ø§Ø± Ú†Ø§Ø±Øª IE Ùˆ OE**
    
    # Calculate metrics grouped by Product
    metrics_by_product = calculate_metrics(prod_df_filtered, err_df_filtered, group_cols=['Product', 'ProductionTypeForTon'])

    if not metrics_by_product.empty:
        st.subheader("Ù…Ù‚Ø§ÛŒØ³Ù‡ Line Efficiency (IE) Ùˆ OE Ø¨Ø± Ø§Ø³Ø§Ø³ Ù…Ø­ØµÙˆÙ„ (Bar Chart)")
        
        fig_product_metrics = px.bar(
            metrics_by_product.sort_values(by='Line_Efficiency(%)', ascending=False),
            x='Product',
            y=['Line_Efficiency(%)', 'OE(%)'],
            title='Ù…Ù‚Ø§ÛŒØ³Ù‡ Line Efficiency (IE) Ùˆ OE Ø¨Ø± Ø§Ø³Ø§Ø³ Ù…Ø­ØµÙˆÙ„',
            height=500,
            barmode='group',
            labels={'Line_Efficiency(%)': 'Line Efficiency (IE) (%)', 'OE(%)': 'OE (%)'},
            template="plotly_dark"
        )
        st.plotly_chart(fig_product_metrics, use_container_width=True)

    st.markdown("---")

    # --- 4. Downtime Error Breakdown (New Chart) ---
    # **Ø¯Ø±Ø®ÙˆØ§Ø³Øª Û´: Ø§ÙØ²ÙˆØ¯Ù† Ù†Ù…Ø§ÛŒØ´ Ø®Ø·Ø§Ù‡Ø§ÛŒ Down Time**
    if not err_df_filtered.empty and 'Error' in err_df_filtered.columns and 'Duration' in err_df_filtered.columns:
        st.subheader("ØªØ¬Ø²ÛŒÙ‡ Ùˆ ØªØ­Ù„ÛŒÙ„ Ú©Ø¯ Ø®Ø·Ø§ÛŒ ØªÙˆÙ‚Ù (Downtime Error Code Breakdown) - Ø¨Ø± Ø­Ø³Ø¨ Ø¯Ù‚ÛŒÙ‚Ù‡")
        
        # Aggregate raw error data by code
        error_breakdown = err_df_filtered.groupby('Error')['Duration'].sum().reset_index()
        
        # Ensure 'Error' is treated as a string for grouping and visualization
        error_breakdown['Error Code'] = error_breakdown['Error'].astype(str)
        
        # Filter out rows with zero duration for cleaner chart
        error_breakdown = error_breakdown[error_breakdown['Duration'] > 0].sort_values(by='Duration', ascending=False)

        fig_error = px.bar(
            error_breakdown,
            x='Error Code',
            y='Duration',
            color='Error Code',
            title='Ù…Ø¯Øª Ø²Ù…Ø§Ù† Ú©Ù„ Ú©Ø¯Ù‡Ø§ÛŒ Ø®Ø·Ø§ÛŒ ØªÙˆÙ‚Ù Ùˆ Ø§ØªÙ„Ø§Ù (Ø¨Ø± Ø­Ø³Ø¨ Ø¯Ù‚ÛŒÙ‚Ù‡)',
            labels={'Duration': 'Ù…Ø¯Øª Ø²Ù…Ø§Ù† Ú©Ù„ (Ø¯Ù‚ÛŒÙ‚Ù‡)', 'Error Code': 'Ú©Ø¯ Ø®Ø·Ø§'},
            hover_data={'Duration': ':.1f'},
            height=500,
            template="plotly_dark"
        )
        st.plotly_chart(fig_error, use_container_width=True)
    elif st.session_state.err_files_loaded:
        st.info("Ù‡ÛŒÚ† Ø¯Ø§Ø¯Ù‡ Ø®Ø·Ø§ÛŒÛŒ Ù…Ù†Ø·Ø¨Ù‚ Ø¨Ø§ ÙÛŒÙ„ØªØ±Ù‡Ø§ÛŒ Ù…Ø§Ø´ÛŒÙ† Ùˆ ØªØ§Ø±ÛŒØ® ÙØ¹Ù„ÛŒ Ù¾ÛŒØ¯Ø§ Ù†Ø´Ø¯.")
    elif not st.session_state.err_files_loaded:
        st.info("Ù„Ø·ÙØ§Ù‹ ÙØ§ÛŒÙ„â€ŒÙ‡Ø§ÛŒ Ø®Ø·Ø§ÛŒ Ø®ÙˆØ¯ Ø±Ø§ Ø¨Ø±Ø§ÛŒ Ù†Ù…Ø§ÛŒØ´ Ø§ÛŒÙ† Ù†Ù…ÙˆØ¯Ø§Ø± Ø¯Ø± Ø¨Ø®Ø´ Data Management Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ Ú©Ù†ÛŒØ¯.")
    
    # --- 5. Removed Chart ---
    # **Ø¯Ø±Ø®ÙˆØ§Ø³Øª Û±: Ø­Ø°Ù Ù†Ù…ÙˆØ¯Ø§Ø± 'daily line efficiency and oe trend for machine'**
    # Ø§ÛŒÙ† Ú©Ø¯Ù‡Ø§ Ø¯Ø± Ø§ÛŒÙ†Ø¬Ø§ Ø­Ø°Ù Ø´Ø¯Ù‡â€ŒØ§Ù†Ø¯.
    st.markdown("---")

def Trend_Analysis():
    st.title("ğŸ“ˆ ØªØ­Ù„ÛŒÙ„ Ø±ÙˆÙ†Ø¯")
    st.markdown("---")

    if st.session_state.prod_df.empty:
        st.info("Ù„Ø·ÙØ§Ù‹ ÙØ§ÛŒÙ„â€ŒÙ‡Ø§ÛŒ ØªÙˆÙ„ÛŒØ¯ Ø®ÙˆØ¯ Ø±Ø§ Ø¯Ø± Ø¨Ø®Ø´ Data Management Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ Ú©Ù†ÛŒØ¯.")
        return

    final_prod_df = st.session_state.prod_df.copy()
    final_err_df = st.session_state.err_df.copy()

    # --- Filters (similar to Dashboard) ---
    with st.expander("ÙÛŒÙ„ØªØ±Ù‡Ø§ÛŒ Ø±ÙˆÙ†Ø¯", expanded=True):
        col1, col2 = st.columns(2)
        
        all_machines = final_prod_df['MachineType'].unique().tolist() if 'MachineType' in final_prod_df.columns else []
        selected_machine = col1.selectbox("Ø§Ù†ØªØ®Ø§Ø¨ Ù…Ø§Ø´ÛŒÙ† Ø¨Ø±Ø§ÛŒ ØªØ­Ù„ÛŒÙ„ Ø±ÙˆÙ†Ø¯:", ["All"] + all_machines)

        min_date = final_prod_df['Date'].min() if not final_prod_df.empty and 'Date' in final_prod_df.columns else datetime.now().date()
        max_date = final_prod_df['Date'].max() if not final_prod_df.empty and 'Date' in final_prod_df.columns else datetime.now().date()

        date_range = col2.date_input("Ù…Ø­Ø¯ÙˆØ¯Ù‡ ØªØ§Ø±ÛŒØ® Ø±ÙˆÙ†Ø¯:", value=(min_date, max_date) if min_date != max_date else (min_date, min_date), min_value=min_date, max_value=max_date)

    # --- Apply Filters ---
    prod_df_filtered = final_prod_df.copy()

    if selected_machine != "All":
        prod_df_filtered = prod_df_filtered[prod_df_filtered['MachineType'] == selected_machine].copy()
        
        # --- FIX: Key Error for MachineType ---
        # **Ø±ÙØ¹ Ø®Ø·Ø§ÛŒ KeyError: 'MachineType' Ø¯Ø± Trend Analysis**
        if 'MachineType' in final_err_df.columns:
            # Ø§ÛŒÙ† Ø®Ø· Ù‡Ù…Ø§Ù† Ø®Ø· Û¹Û³Û¸ Ø¯Ø± Traceback Ø§ØµÙ„ÛŒ Ø¨ÙˆØ¯. Ø¨Ø§ Ø¨Ø±Ø±Ø³ÛŒ ÙˆØ¬ÙˆØ¯ Ø³ØªÙˆÙ†ØŒ Ø§Ø² Ø®Ø·Ø§ Ø¬Ù„ÙˆÚ¯ÛŒØ±ÛŒ Ù…ÛŒâ€ŒØ´ÙˆØ¯.
            final_err_df_filtered = final_err_df[
                final_err_df["MachineType"] == selected_machine
            ].copy()
        else:
            final_err_df_filtered = pd.DataFrame() # Ø§Ú¯Ø± Ø³ØªÙˆÙ† Ù†Ø¨ÙˆØ¯ØŒ Ø¯ÛŒØªØ§ÙØ±ÛŒÙ… Ø®Ø§Ù„ÛŒ Ø§ÛŒØ¬Ø§Ø¯ Ø´ÙˆØ¯
        # --- END FIX ---
    else:
        final_err_df_filtered = final_err_df.copy()

    if len(date_range) == 2:
        start_date, end_date = date_range[0], date_range[1]
        prod_df_filtered = prod_df_filtered[
            (prod_df_filtered['Date'] >= start_date) & 
            (prod_df_filtered['Date'] <= end_date)
        ].copy()
        
        if 'Date' in final_err_df_filtered.columns:
            final_err_df_filtered = final_err_df_filtered[
                (final_err_df_filtered['Date'] >= start_date) & 
                (final_err_df_filtered['Date'] <= end_date)
            ].copy()

    # --- Daily Trend Calculation ---
    daily_metrics_df = calculate_metrics(prod_df_filtered, final_err_df_filtered, group_cols=['Date'])
    
    if not daily_metrics_df.empty:
        daily_metrics_df = daily_metrics_df.sort_values(by='Date')

        # --- Daily OEE/Availability Trend ---
        st.subheader("Ø±ÙˆÙ†Ø¯ Ø±ÙˆØ²Ø§Ù†Ù‡ OEEØŒ Ø¯Ø± Ø¯Ø³ØªØ±Ø³ Ø¨ÙˆØ¯Ù† Ùˆ Ø±Ø§Ù†Ø¯Ù…Ø§Ù† Ø®Ø·")
        fig_trend = px.line(
            daily_metrics_df,
            x='Date',
            y=['OE(%)', 'Availability(%)', 'Line_Efficiency(%)'],
            title=f"Ø±ÙˆÙ†Ø¯ Ø±ÙˆØ²Ø§Ù†Ù‡ OEE Ùˆ Ù…ØªØ±ÛŒÚ©â€ŒÙ‡Ø§ Ø¨Ø±Ø§ÛŒ {selected_machine}",
            labels={'value': 'Ø¯Ø±ØµØ¯ (%)', 'variable': 'Ù…ØªØ±ÛŒÚ©'},
            template="plotly_dark",
            markers=True
        )
        fig_trend.update_layout(hovermode="x unified")
        st.plotly_chart(fig_trend, use_container_width=True)

        st.markdown("---")

        # --- Daily Downtime Trend ---
        if not final_err_df_filtered.empty and 'Duration' in final_err_df_filtered.columns:
            st.subheader("Ø±ÙˆÙ†Ø¯ Ø±ÙˆØ²Ø§Ù†Ù‡ Ù…Ø¯Øª Ø²Ù…Ø§Ù† ØªÙˆÙ‚Ù (Downtime) Ø¨Ø± Ø­Ø³Ø¨ Ø¯Ù‚ÛŒÙ‚Ù‡")
            daily_downtime = final_err_df_filtered.groupby('Date')['Duration'].sum().reset_index()
            daily_downtime = daily_downtime.sort_values(by='Date')

            fig_downtime = px.bar(
                daily_downtime,
                x='Date',
                y='Duration',
                title=f"Ø±ÙˆÙ†Ø¯ Ø±ÙˆØ²Ø§Ù†Ù‡ Ú©Ù„ Ø²Ù…Ø§Ù† ØªÙˆÙ‚Ù Ø¨Ø±Ø§ÛŒ {selected_machine}",
                labels={'Duration': 'Ù…Ø¯Øª Ø²Ù…Ø§Ù† Ú©Ù„ ØªÙˆÙ‚Ù (Ø¯Ù‚ÛŒÙ‚Ù‡)'},
                template="plotly_dark"
            )
            st.plotly_chart(fig_downtime, use_container_width=True)
        
    else:
        st.warning("Ø¯Ø§Ø¯Ù‡â€ŒØ§ÛŒ Ø¨Ø±Ø§ÛŒ Ù†Ù…Ø§ÛŒØ´ Ø±ÙˆÙ†Ø¯ Ø¯Ø± Ù…Ø­Ø¯ÙˆØ¯Ù‡ ÙÛŒÙ„ØªØ±Ù‡Ø§ÛŒ Ø§Ù†ØªØ®Ø§Ø¨ÛŒ Ù¾ÛŒØ¯Ø§ Ù†Ø´Ø¯.")


def Data_Management():
    st.title("ğŸ“‚ Ù…Ø¯ÛŒØ±ÛŒØª Ø¯Ø§Ø¯Ù‡ Ùˆ Ø¢Ø±Ø´ÛŒÙˆ")
    st.markdown("---")

    # Placeholder for the file uploader and data management logic
    # Uploader for Production Data
    prod_files = st.file_uploader("Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ ÙØ§ÛŒÙ„â€ŒÙ‡Ø§ÛŒ Ø§Ú©Ø³Ù„ ØªÙˆÙ„ÛŒØ¯ (Production Data)", type=['xlsx'], accept_multiple_files=True, key="prod_uploader")
    if prod_files:
        if st.button("Ù¾Ø±Ø¯Ø§Ø²Ø´ Ùˆ Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ ØªÙˆÙ„ÛŒØ¯"):
            with st.spinner("Ø¯Ø± Ø­Ø§Ù„ Ù¾Ø±Ø¯Ø§Ø²Ø´ ÙØ§ÛŒÙ„â€ŒÙ‡Ø§..."):
                combined_df = process_uploaded_files(prod_files, 'production')
                if not combined_df.empty:
                    st.session_state.prod_df = combined_df
                    st.session_state.prod_files_loaded = True
                    st.success(f"ØªØ¹Ø¯Ø§Ø¯ {len(combined_df)} Ø±Ú©ÙˆØ±Ø¯ ØªÙˆÙ„ÛŒØ¯ Ø¨Ø§ Ù…ÙˆÙÙ‚ÛŒØª Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ Ø´Ø¯.")
                else:
                    st.warning("ÙØ§ÛŒÙ„â€ŒÙ‡Ø§ÛŒ ØªÙˆÙ„ÛŒØ¯ ÙØ§Ù‚Ø¯ Ø¯Ø§Ø¯Ù‡ Ù…Ø¹ØªØ¨Ø± Ø¨ÙˆØ¯Ù†Ø¯ ÛŒØ§ ÙØ±Ù…Øª Ø¢Ù†â€ŒÙ‡Ø§ ØµØ­ÛŒØ­ Ù†Ø¨ÙˆØ¯.")

    st.markdown("---")
    
    # Uploader for Error Data
    err_files = st.file_uploader("Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ ÙØ§ÛŒÙ„â€ŒÙ‡Ø§ÛŒ Ø§Ú©Ø³Ù„ Ø®Ø·Ø§ (Error Data)", type=['xlsx'], accept_multiple_files=True, key="err_uploader")
    if err_files:
        if st.button("Ù¾Ø±Ø¯Ø§Ø²Ø´ Ùˆ Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ Ø®Ø·Ø§"):
            with st.spinner("Ø¯Ø± Ø­Ø§Ù„ Ù¾Ø±Ø¯Ø§Ø²Ø´ ÙØ§ÛŒÙ„â€ŒÙ‡Ø§ÛŒ Ø®Ø·Ø§..."):
                combined_err_df = process_uploaded_files(err_files, 'error')
                if not combined_err_df.empty:
                    st.session_state.err_df = combined_err_df
                    st.session_state.err_files_loaded = True
                    st.success(f"ØªØ¹Ø¯Ø§Ø¯ {len(combined_err_df)} Ø±Ú©ÙˆØ±Ø¯ Ø®Ø·Ø§ Ø¨Ø§ Ù…ÙˆÙÙ‚ÛŒØª Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ Ø´Ø¯.")
                else:
                    st.warning("ÙØ§ÛŒÙ„â€ŒÙ‡Ø§ÛŒ Ø®Ø·Ø§ ÙØ§Ù‚Ø¯ Ø¯Ø§Ø¯Ù‡ Ù…Ø¹ØªØ¨Ø± Ø¨ÙˆØ¯Ù†Ø¯ ÛŒØ§ ÙØ±Ù…Øª Ø¢Ù†â€ŒÙ‡Ø§ ØµØ­ÛŒØ­ Ù†Ø¨ÙˆØ¯.")

    st.markdown("---")
    
    st.subheader("ÙˆØ¶Ø¹ÛŒØª Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ ÙØ¹Ù„ÛŒ")
    col_d1, col_d2 = st.columns(2)
    col_d1.metric("ØªØ¹Ø¯Ø§Ø¯ Ø±Ú©ÙˆØ±Ø¯ ØªÙˆÙ„ÛŒØ¯", len(st.session_state.prod_df))
    col_d2.metric("ØªØ¹Ø¯Ø§Ø¯ Ø±Ú©ÙˆØ±Ø¯ Ø®Ø·Ø§", len(st.session_state.err_df))
    
    # Placeholder for archiving and deletion logic (using Supabase config)
    st.markdown("---")
    st.subheader("Ø¢Ø±Ø´ÛŒÙˆ (Supabase)")
    
    # New logic to list and display archived files
    if st.button("ØªØ§Ø²Ù‡â€ŒØ³Ø§Ø²ÛŒ Ùˆ Ù†Ù…Ø§ÛŒØ´ Ù„ÛŒØ³Øª Ø¢Ø±Ø´ÛŒÙˆ", key="refresh_archive"):
        with st.spinner("Ø¯Ø± Ø­Ø§Ù„ Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ Ù„ÛŒØ³Øª ÙØ§ÛŒÙ„â€ŒÙ‡Ø§..."):
            st.session_state.archived_files = list_archived_files()
        
    if st.session_state.archived_files:
        st.success(f"ØªØ¹Ø¯Ø§Ø¯ {len(st.session_state.archived_files)} ÙØ§ÛŒÙ„ Ø¯Ø± Ø¢Ø±Ø´ÛŒÙˆ ÛŒØ§ÙØª Ø´Ø¯.")
        # Displaying the list of files in a clean dataframe
        df_files = pd.DataFrame({'Ù†Ø§Ù… ÙØ§ÛŒÙ„ Ø¢Ø±Ø´ÛŒÙˆ Ø´Ø¯Ù‡': st.session_state.archived_files})
        st.dataframe(df_files, use_container_width=True, height=200)
    else:
        st.info("Ù‡ÛŒÚ† ÙØ§ÛŒÙ„ÛŒ Ø¯Ø± Ø¢Ø±Ø´ÛŒÙˆ ÛŒØ§ÙØª Ù†Ø´Ø¯. Ø¨Ø±Ø§ÛŒ Ø¨Ù‡â€ŒØ±ÙˆØ²Ø±Ø³Ø§Ù†ÛŒ Ù„ÛŒØ³ØªØŒ Ø¯Ú©Ù…Ù‡ ØªØ§Ø²Ù‡â€ŒØ³Ø§Ø²ÛŒ Ø±Ø§ Ø¨Ø²Ù†ÛŒØ¯.")

def Contact_Me():
    st.subheader("Ø§Ø±ØªØ¨Ø§Ø· Ø¨Ø§ Ù…Ø­Ù…Ø¯ Ø§Ø³Ø¯Ø§Ù„Ù„Ù‡â€ŒØ²Ø§Ø¯Ù‡")
    st.markdown("---")
    st.markdown("""
    Ø¯Ø± Ø¯Ù†ÛŒØ§ÛŒ Ù¾Ø±Ø´ØªØ§Ø¨ Ø§Ù…Ø±ÙˆØ²ØŒ Ø¨Ø§ Ù¾ÛŒØ´Ø±ÙØªâ€ŒÙ‡Ø§ÛŒ Ø³Ø±ÛŒØ¹ Ø¯Ø± ÙÙ†Ø§ÙˆØ±ÛŒØŒ Ù‡ÙˆØ´ Ù…ØµÙ†ÙˆØ¹ÛŒ Ø¯ÛŒÚ¯Ø± ÛŒÚ© Ú¯Ø²ÛŒÙ†Ù‡ Ù†ÛŒØ³Øªâ€”Ø¨Ù„Ú©Ù‡ ÛŒÚ© Ø¶Ø±ÙˆØ±Øª Ø§Ø³Øª. Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² Ù‡ÙˆØ´ Ù…ØµÙ†ÙˆØ¹ÛŒ Ù…ÛŒâ€ŒØªÙˆØ§Ù†Ø¯ Ø¨Ù‡ Ø·ÙˆØ± Ù‚Ø§Ø¨Ù„ ØªÙˆØ¬Ù‡ÛŒ Ø¹Ù…Ù„Ú©Ø±Ø¯ Ø±Ø§ Ø§ÙØ²Ø§ÛŒØ´ Ø¯Ø§Ø¯Ù‡ØŒ Ø®Ø·Ø§Ù‡Ø§ÛŒ Ø§Ù†Ø³Ø§Ù†ÛŒ Ø±Ø§ Ø¨Ù‡ Ø­Ø¯Ø§Ù‚Ù„ Ø¨Ø±Ø³Ø§Ù†Ø¯ Ùˆ Ø¬Ø±ÛŒØ§Ù† Ú©Ø§Ø± Ø±Ø§ Ø³Ø§Ø¯Ù‡â€ŒØªØ± Ú©Ù†Ø¯. ØªÚ©ÛŒÙ‡ ØµØ±Ù Ø¨Ù‡ Ø±ÙˆØ´â€ŒÙ‡Ø§ÛŒ Ø³Ù†ØªÛŒ Ø§ØºÙ„Ø¨ Ù…Ù†Ø¬Ø± Ø¨Ù‡ Ø§ØªÙ„Ø§Ù Ø²Ù…Ø§Ù† Ùˆ ØªÙ„Ø§Ø´ Ù…ÛŒâ€ŒØ´ÙˆØ¯ØŒ Ø¨Ø¯ÙˆÙ† Ø§ÛŒÙ†Ú©Ù‡ Ú©Ø§Ø±Ø§ÛŒÛŒ Ù…ÙˆØ±Ø¯ Ù†Ø¸Ø±Ù…Ø§Ù† Ø±Ø§ Ø¨Ù‡ Ø¯Ø³Øª Ø¢ÙˆØ±ÛŒÙ….

    Ø¨Ø±Ø§ÛŒ Ù¾Ø±Ø¯Ø§Ø®ØªÙ† Ø¨Ù‡ Ø§ÛŒÙ† Ù…ÙˆØ¶ÙˆØ¹ØŒ Ù…Ù† Ø´Ø±ÙˆØ¹ Ø¨Ù‡ Ø³Ø§Ø®Øª Ù¾Ù„ØªÙØ±Ù…ÛŒ Ú©Ø±Ø¯Ù‡â€ŒØ§Ù… Ú©Ù‡ Ø§ØªÙˆÙ…Ø§Ø³ÛŒÙˆÙ† Ø±Ø§ Ø¨Ø§ Ù‡ÙˆØ´Ù…Ù†Ø¯ÛŒ ØªØ±Ú©ÛŒØ¨ Ù…ÛŒâ€ŒÚ©Ù†Ø¯. Ø¨Ø§ Ø§Ù†Ú¯ÛŒØ²Ù‡ Ø§Ø´ØªÛŒØ§Ù‚Ù… Ø¨Ù‡ Ù¾Ø§ÛŒØªÙˆÙ†â€”Ø¨Ø§ ÙˆØ¬ÙˆØ¯ Ø§ÛŒÙ†Ú©Ù‡ Ù‡Ù†ÙˆØ² Ø¯Ø± Ø­Ø§Ù„ ÛŒØ§Ø¯Ú¯ÛŒØ±ÛŒ Ù‡Ø³ØªÙ…â€”Ùˆ Ø¹Ù„Ø§Ù‚Ù‡ Ø¹Ù…ÛŒÙ‚ Ø¨Ù‡ Ø§ÛŒØ¬Ø§Ø¯ Ø±Ø§Ù‡Ú©Ø§Ø±Ù‡Ø§ÛŒ ÙÙ†ÛŒ Ù…Ù†Ø¶Ø¨Ø· Ùˆ Ù…Ø¨ØªÙ†ÛŒ Ø¨Ø± Ø¯Ø§Ø¯Ù‡ØŒ ØªÙˆØ³Ø¹Ù‡ Ø§ÛŒÙ† ÙˆØ¨â€ŒØ³Ø§ÛŒØª Ù…Ø¨ØªÙ†ÛŒ Ø¨Ø± Streamlit Ø±Ø§ Ø¨Ø±Ø§ÛŒ ØªØ­Ù„ÛŒÙ„ Ø¹Ù…Ù„Ú©Ø±Ø¯ Ø±ÙˆØ²Ø§Ù†Ù‡ ØªÙˆÙ„ÛŒØ¯ Ø¢ØºØ§Ø² Ú©Ø±Ø¯Ù….

    Ø¯Ø± Ø­Ø§Ù„ÛŒ Ú©Ù‡ Ù…Ù‡Ø§Ø±Øªâ€ŒÙ‡Ø§ÛŒ Ù¾Ø§ÛŒØªÙˆÙ† Ù…Ù† Ù‡Ù†ÙˆØ² Ø¯Ø± Ø­Ø§Ù„ Ø±Ø´Ø¯ Ù‡Ø³ØªÙ†Ø¯ØŒ ØµØ¨Ø±ØŒ ØªØ¹Ù‡Ø¯ Ùˆ Ú©Ù†Ø¬Ú©Ø§ÙˆÛŒ Ø±Ø§ Ø¨Ù‡ Ú©Ø§Ø± Ú¯Ø±ÙØªÙ‡â€ŒØ§Ù…. Ø¯Ø± Ø·ÙˆÙ„ ÙØ±Ø¢ÛŒÙ†Ø¯ØŒ Ø§Ø¨Ø²Ø§Ø±Ù‡Ø§ÛŒÛŒ Ù…Ø§Ù†Ù†Ø¯ **Gemini AI** Ø¯Ø± Ú©Ù…Ú© Ø¨Ù‡ Ù…Ù† Ø¨Ø±Ø§ÛŒ Ø§Ø´Ú©Ø§Ù„â€ŒØ²Ø¯Ø§ÛŒÛŒØŒ Ù¾Ø§Ù„Ø§ÛŒØ´ Ø§Ø³ØªØ±Ø§ØªÚ˜ÛŒâ€ŒÙ‡Ø§ Ùˆ Ø¨Ù‡ Ø«Ù…Ø± Ø±Ø³Ø§Ù†Ø¯Ù† Ø§ÛŒÙ† Ø§ÛŒØ¯Ù‡ Ø¨Ø³ÛŒØ§Ø± Ù…Ø¤Ø«Ø± Ø¨ÙˆØ¯Ù†Ø¯. ØµØ§Ø¯Ù‚Ø§Ù†Ù‡ Ø¨Ú¯ÙˆÛŒÙ…ØŒ Ø¨Ø¯ÙˆÙ† Ú©Ù…Ú© Ù‡ÙˆØ´ Ù…ØµÙ†ÙˆØ¹ÛŒØŒ Ø±Ø³ÛŒØ¯Ù† Ø¨Ù‡ Ø§ÛŒÙ† Ù†Ù‚Ø·Ù‡ Ø¨Ø³ÛŒØ§Ø± Ø¯Ø´ÙˆØ§Ø±ØªØ± Ù…ÛŒâ€ŒØ¨ÙˆØ¯.

    Ø¨Ø§ Ø§ÛŒÙ† Ø­Ø§Ù„ØŒ Ù…Ù† Ù…ØªØ¹Ù‡Ø¯ Ø¨Ù‡ Ø¨Ù‡Ø¨ÙˆØ¯ Ù‡Ø³ØªÙ…â€”Ù‡Ù… Ø¯Ø± Ú©Ø¯Ù†ÙˆÛŒØ³ÛŒ Ùˆ Ù‡Ù… Ø¯Ø± Ø·Ø±Ø§Ø­ÛŒ Ø³ÛŒØ³ØªÙ…. Ù…Ù† Ø§Ø² Ø¨Ø§Ø²Ø®ÙˆØ±Ø¯ØŒ Ù¾ÛŒØ´Ù†Ù‡Ø§Ø¯Ø§Øª ÛŒØ§ Ù‡Ø± Ø±Ø§Ù‡Ù†Ù…Ø§ÛŒÛŒ Ø´Ù…Ø§ Ø¨Ø±Ø§ÛŒ Ú©Ù…Ú© Ø¨Ù‡ Ø§Ø±ØªÙ‚Ø§ÛŒ Ø¨ÛŒØ´ØªØ± Ø§ÛŒÙ† Ù¾Ù„ØªÙØ±Ù… Ø§Ø³ØªÙ‚Ø¨Ø§Ù„ Ù…ÛŒâ€ŒÚ©Ù†Ù….

    ğŸ“§ Ø§ÛŒÙ…ÛŒÙ„: m.asdz@yahoo.com
    ğŸ”— Ù„ÛŒÙ†Ú©Ø¯ÛŒÙ†: Mohammad Asdollahzadeh

    Ø§Ø² Ø¨Ø§Ø²Ø¯ÛŒØ¯ Ø´Ù…Ø§ Ù…ØªØ´Ú©Ø±Ù… Ùˆ Ø§Ø² Ø­Ù…Ø§ÛŒØª Ø´Ù…Ø§ ØµÙ…ÛŒÙ…Ø§Ù†Ù‡ Ù‚Ø¯Ø±Ø¯Ø§Ù†ÛŒ Ù…ÛŒâ€ŒÚ©Ù†Ù….

    Ø¨Ø§ Ø§Ø­ØªØ±Ø§Ù…ØŒ
    Ù…Ø­Ù…Ø¯ Ø§Ø³Ø¯Ø§Ù„Ù„Ù‡â€ŒØ²Ø§Ø¯Ù‡
    """)


# --- Page Routing ---
if st.session_state.page == "Dashboard":
    Data_Analyzing_Dashboard()
elif st.session_state.page == "Trend Analysis":
    Trend_Analysis()
elif st.session_state.page == "Data Management":
    Data_Management()
elif st.session_state.page == "Contact Me":
    Contact_Me()
