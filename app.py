import streamlit as st
import pandas as pd
import plotly.express as px
import plotly.graph_objects as go
from io import BytesIO
from supabase import create_client, Client
import base64
from datetime import datetime, timedelta, time as datetime_time
import re
import time
import numpy as np
import json
import logging

# ØªÙ†Ø¸ÛŒÙ…Ø§Øª Ù„Ø§Ú¯ÛŒÙ†Ú¯ (Ø§Ø®ØªÛŒØ§Ø±ÛŒ)
# logging.basicConfig(level=logging.INFO)

# --- Supabase Configuration ---
# ØªÙˆØ¬Ù‡: Ú©Ù„ÛŒØ¯Ù‡Ø§ÛŒ Ø´Ù…Ø§ Ø­ÙØ¸ Ø´Ø¯Ù‡â€ŒØ§Ù†Ø¯.
SUPABASE_URL = "https://rlutsxvghmhrgcnqbmch.supabase.co"
SUPABASE_KEY = "eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJzdXBhYmFzZSIsInJlZiI6InJsdXRzeHZnaG1ocmdjbnFibWNoIiwicm9sZSI6InNlcnZpY2Vfcm9sZSIsImlhdCI6MTc0NTEyODk5MSwiZXhwIjoyMDYwNzA0OTkxfQ.VPxJbrPUw4E-MyRGklQMcxveUTznNlWLhPO-mqrHv9c"

# Initialize Supabase client globally
supabase: Client = create_client(SUPABASE_URL, SUPABASE_KEY)

# --- Password for Archive Deletion ---
ARCHIVE_DELETE_PASSWORD = "beautifulmind"

# --- DB Table Names ---
PROD_TABLE = "production_data"
ERROR_TABLE = "error_data"
# -------------------------


st.set_page_config(layout="wide", page_title="OEE & Production Dashboard", initial_sidebar_state="expanded")
st.title("ğŸ“Š Ø¯Ø§Ø´Ø¨ÙˆØ±Ø¯ ØªØ­Ù„ÛŒÙ„ ØªÙˆÙ„ÛŒØ¯ Ùˆ OEE")


# --- Helper Functions (Updated/New) ---

@st.cache_data(ttl=3600, show_spinner="Ø¯Ø±ÛŒØ§ÙØª Ø§Ø·Ù„Ø§Ø¹Ø§Øª Ø§Ø² Ù¾Ø§ÛŒÚ¯Ø§Ù‡ Ø¯Ø§Ø¯Ù‡...")
def load_data_from_supabase_tables(table_name):
    """Fetches all data from a specified Supabase PostgreSQL table."""
    try:
        response = supabase.table(table_name).select("*").order("Date", desc=False).execute()
        data = response.data
        if not data:
            return pd.DataFrame()
        
        df = pd.DataFrame(data)
        # Convert necessary columns to correct types
        if 'Date' in df.columns:
            df['Date'] = pd.to_datetime(df['Date']).dt.date
        
        # Ensure numeric columns are correct
        for col in ['Duration', 'PackQty', 'Waste', 'Ton', 'Capacity', 'Manpower']:
            if col in df.columns:
                df[col] = pd.to_numeric(df[col], errors='coerce').fillna(0)
                
        return df

    except Exception as e:
        # Check if the error is due to missing table (42P01: relation does not exist)
        if isinstance(e, dict) and e.get('code') == '42P01':
            st.error(f"Ø®Ø·Ø§ÛŒ Ù¾Ø§ÛŒÚ¯Ø§Ù‡ Ø¯Ø§Ø¯Ù‡: Ø¬Ø¯ÙˆÙ„ '{table_name}' ÙˆØ¬ÙˆØ¯ Ù†Ø¯Ø§Ø±Ø¯. Ù„Ø·ÙØ§Ù‹ Ù…Ø·Ù…Ø¦Ù† Ø´ÙˆÛŒØ¯ Ø¬Ø¯Ø§ÙˆÙ„ Ø±Ø§ Ø¯Ø± Supabase Ø§ÛŒØ¬Ø§Ø¯ Ú©Ø±Ø¯Ù‡â€ŒØ§ÛŒØ¯.")
        elif '42P01' in str(e): # General check for 42P01 if error format is different
             st.error(f"Ø®Ø·Ø§ÛŒ Ù¾Ø§ÛŒÚ¯Ø§Ù‡ Ø¯Ø§Ø¯Ù‡: Ø¬Ø¯ÙˆÙ„ '{table_name}' ÙˆØ¬ÙˆØ¯ Ù†Ø¯Ø§Ø±Ø¯. Ù„Ø·ÙØ§Ù‹ Ù…Ø·Ù…Ø¦Ù† Ø´ÙˆÛŒØ¯ Ø¬Ø¯Ø§ÙˆÙ„ Ø±Ø§ Ø¯Ø± Supabase Ø§ÛŒØ¬Ø§Ø¯ Ú©Ø±Ø¯Ù‡â€ŒØ§ÛŒØ¯.")
        else:
            st.error(f"Ø®Ø·Ø§ Ø¯Ø± Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ Ø§Ø² Supabase Ø¨Ø±Ø§ÛŒ Ø¬Ø¯ÙˆÙ„ {table_name}: {e}")
        return pd.DataFrame()

def insert_to_db(df, table_name):
    """Inserts DataFrame records into the specified Supabase table."""
    if df.empty:
        return True
    try:
        # Convert DataFrame to list of dictionaries
        data_to_insert = df.to_dict('records')
        
        # Use upsert to handle potential duplicates (based on a unique key if one exists, 
        # but since we don't have a natural key in the Excel data, we just insert).
        # For simplicity, we assume we are not inserting duplicates.
        response = supabase.table(table_name).insert(data_to_insert).execute()
        
        if response.data:
            return True
        else:
            # Supabase API can return an error dictionary even if 'execute' succeeds but insertion fails
            error_data = getattr(response, 'error', None)
            if error_data:
                 st.error(f"Ø®Ø·Ø§ Ø¯Ø± Ø¯Ø±Ø¬ Ø¯Ø§Ø¯Ù‡ Ø¨Ù‡ Ø¬Ø¯ÙˆÙ„ {table_name}: {error_data.get('message', 'Ø®Ø·Ø§ÛŒ Ù†Ø§Ù…Ø´Ø®Øµ')}")
                 return False
            return True # Assume success if no error is explicitly returned

    except Exception as e:
        st.error(f"Ø®Ø·Ø§ÛŒ Ú©Ù„ÛŒ Ø¯Ø± Ø¯Ø±Ø¬ Ø¯Ø§Ø¯Ù‡ Ø¨Ù‡ Ø¬Ø¯ÙˆÙ„ {table_name}: {e}")
        return False

# --- OEE and Analysis Metrics (New/Replaced Logic) ---

def calculate_oee_metrics(df_prod, df_err):
    """Calculates OEE, Availability, Performance, and Quality metrics."""
    if df_prod.empty:
        return 0, 0, 0, 0, 0, 0, 0, 0 # Returns default KPIs

    # --- 1. Planned Production Time (Total Duration) ---
    # Duration is in hours, convert to minutes
    total_planned_time_min = df_prod["Duration"].sum() * 60

    # --- 2. Down Time (Error Time) ---
    # df_err['Duration'] is already in minutes
    total_down_time_min = df_err["Duration"].sum()
    
    # --- 3. Operating Time ---
    operating_time_min = total_planned_time_min - total_down_time_min
    operating_time_min = max(0, operating_time_min) # Cannot be negative

    # --- KPI 1: Availability (%) ---
    availability_pct = 0
    if total_planned_time_min > 0:
        availability_pct = (operating_time_min / total_planned_time_min) * 100
    
    # --- 4. Total Production (Packages) ---
    total_pack_qty = df_prod["PackQty"].sum()
    total_waste = df_prod["Waste"].sum()
    total_good_qty = total_pack_qty - total_waste

    # --- KPI 2: Quality (%) ---
    quality_pct = 0
    if total_pack_qty > 0:
        quality_pct = (total_good_qty / total_pack_qty) * 100
    
    # --- 5. Ideal Cycle Rate (Capacity) ---
    # We use the average recorded Capacity (units/hour) for the time period
    avg_capacity_units_per_hour = df_prod["Capacity"].mean() 
    
    ideal_cycle_rate_per_min = avg_capacity_units_per_hour / 60 if avg_capacity_units_per_hour > 0 else 0
        
    # --- 6. Theoretical Production Time (Ideal Run Time) ---
    # Theoretical Production Time (Ideal Time to produce actual total_pack_qty)
    theoretical_run_time_min = 0 
    if ideal_cycle_rate_per_min > 0:
        theoretical_run_time_min = total_pack_qty / ideal_cycle_rate_per_min
        
    # --- KPI 3: Performance (%) ---
    performance_pct = 0
    if operating_time_min > 0:
        performance_pct = (theoretical_run_time_min / operating_time_min) * 100
        # Performance should not exceed 100% (Cap it for OEE standard)
        performance_pct = min(performance_pct, 100) 
        
    # --- KPI 4: OEE (%) ---
    oee_pct = (availability_pct / 100) * (performance_pct / 100) * (quality_pct / 100) * 100
    
    # --- KPI 5: Line Efficiency (Total Yield % against Theoretical Max) ---
    # Total Potential Packages = Total Planned Time (min) * Ideal Cycle Rate (units/min)
    total_potential_packages = total_planned_time_min * ideal_cycle_rate_per_min
    line_efficiency_pct = 0
    if total_potential_packages > 0:
        line_efficiency_pct = (total_good_qty / total_potential_packages) * 100
        line_efficiency_pct = min(line_efficiency_pct, 100)
    
    
    return oee_pct, line_efficiency_pct, availability_pct, performance_pct, quality_pct, total_down_time_min, total_good_qty, total_pack_qty

# --- Helper functions from original code (kept for consistency) ---

# All original helper functions (parse_filename_date_to_datetime, upload_to_supabase, 
# get_all_supabase_files, download_from_supabase, get_download_link, convert_time, 
# convert_duration_to_minutes, calculate_ton, determine_machine_type, read_production_data, 
# read_error_data, clear_supabase_bucket) should be included here. 
# Due to the length, I will only include the necessary modifications to read_production_data and read_error_data.

# NOTE: The rest of the original helper functions (parse_filename_date_to_datetime to clear_supabase_bucket) 
# must be placed here for the code to run, but are omitted for brevity in this response. 
# Assuming user copies the original functions from their 'test.txt' file, only the modified part is shown.

# **Modification to read_production_data (Removing old efficiency calc and streamlining)**
# This function must be updated to NOT calculate Efficiency(%) and PotentialProduction
def read_production_data(df_raw_sheet, uploaded_file_name, selected_sheet_name, file_date_obj):
    # ... (Keep all lines up to the final column selection) ...
    # Old logic for Efficiency(%) and PotentialProduction is REMOVED
    # (Lines 336-338 in original content)
    
    # Select and order final columns for the output DataFrame
    final_cols = ["Date", "Product", "Capacity", "Manpower", "Duration", "PackQty", "Waste", "Ton", 
                  "ProductionTypeForTon"] # Efficiency columns removed
    data = data[[col for col in final_cols if col in data.columns]]
    return data
    
# **Modification to read_error_data (No change needed, it's fine)**
# def read_error_data(df_raw_sheet, sheet_name_for_debug="Unknown Sheet", uploaded_file_name_for_debug="Unknown File", file_date_obj=None):
#    ... (Keep the original function) ...

# ----------------------------------------------------------------------------------------------------------------

# --- NEW: Master Processing Function for Upload Page ---
def process_and_insert_data(uploaded_files, sheet_name_to_process):
    """Uploads to storage, processes the specified sheet, and inserts data into DB tables."""
    total_files = len(uploaded_files)
    success_count = 0
    
    # First, upload all files to storage (Archive)
    st.markdown("### Û±. Ø¢Ø±Ø´ÛŒÙˆ ÙØ§ÛŒÙ„â€ŒÙ‡Ø§ÛŒ Ø®Ø§Ù… (Storage)")
    upload_to_supabase(uploaded_files) 
    
    # After storage upload, we can now process them from the uploaded file object itself (faster than re-downloading)
    st.markdown("### Û². Ù¾Ø±Ø¯Ø§Ø²Ø´ Ùˆ Ø¯Ø±Ø¬ Ø¯Ø§Ø¯Ù‡ Ø¨Ù‡ Ø¬Ø¯Ø§ÙˆÙ„ (PostgreSQL)")
    status = st.status("Ø¯Ø± Ø­Ø§Ù„ Ù¾Ø±Ø¯Ø§Ø²Ø´ Ùˆ Ø¯Ø±Ø¬ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§...", expanded=True)
    
    for i, file in enumerate(uploaded_files):
        original_filename = file.name
        file_date_obj = parse_filename_date_to_datetime(original_filename)
        
        status.write(f"Ø¯Ø± Ø­Ø§Ù„ Ù¾Ø±Ø¯Ø§Ø²Ø´ ÙØ§ÛŒÙ„: **{original_filename}** (ØªØ§Ø±ÛŒØ®: {file_date_obj})")
        
        try:
            # Read all sheets to find the correct one
            df_raw_sheet = pd.read_excel(BytesIO(file.getvalue()), sheet_name=sheet_name_to_process, header=None)

            # Read Production and Error data
            prod_df = read_production_data(df_raw_sheet, original_filename, sheet_name_to_process, file_date_obj)
            err_df = read_error_data(df_raw_sheet, sheet_name_to_process, original_filename, file_date_obj)

            # Insert Production Data
            if not prod_df.empty:
                prod_success = insert_to_db(prod_df, PROD_TABLE)
                if prod_success:
                    status.write(f"âœ… Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ ØªÙˆÙ„ÛŒØ¯ Ø¨Ø§ Ù…ÙˆÙÙ‚ÛŒØª Ø¨Ù‡ Ø¬Ø¯ÙˆÙ„ `{PROD_TABLE}` Ø¯Ø±Ø¬ Ø´Ø¯.")
                else:
                    status.write(f"âŒ Ø®Ø·Ø§ÛŒ Ø¯Ø±Ø¬ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ ØªÙˆÙ„ÛŒØ¯ Ø¨Ù‡ `{PROD_TABLE}`.")
                    continue # Skip error data insertion if prod data failed
            else:
                status.write(f"âš ï¸ ÙØ§ÛŒÙ„ **{original_filename}** Ø­Ø§ÙˆÛŒ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ ØªÙˆÙ„ÛŒØ¯ Ù…Ø¹ØªØ¨Ø± Ù†Ø¨ÙˆØ¯ ÛŒØ§ Ø®Ø§Ù„ÛŒ Ø¨ÙˆØ¯.")

            # Insert Error Data
            if not err_df.empty:
                err_success = insert_to_db(err_df, ERROR_TABLE)
                if err_success:
                    status.write(f"âœ… Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ Ø®Ø·Ø§ Ø¨Ø§ Ù…ÙˆÙÙ‚ÛŒØª Ø¨Ù‡ Ø¬Ø¯ÙˆÙ„ `{ERROR_TABLE}` Ø¯Ø±Ø¬ Ø´Ø¯.")
                else:
                    status.write(f"âŒ Ø®Ø·Ø§ÛŒ Ø¯Ø±Ø¬ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ Ø®Ø·Ø§ Ø¨Ù‡ `{ERROR_TABLE}`.")
            else:
                 status.write(f"âš ï¸ ÙØ§ÛŒÙ„ **{original_filename}** Ø­Ø§ÙˆÛŒ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ Ø®Ø·Ø§ Ù…Ø¹ØªØ¨Ø± Ù†Ø¨ÙˆØ¯ ÛŒØ§ Ø®Ø§Ù„ÛŒ Ø¨ÙˆØ¯.")

            success_count += 1

        except ValueError as e:
            if 'Worksheet named' in str(e) and 'not found' in str(e):
                status.write(f"âŒ Ø®Ø·Ø§ Ø¯Ø± Ù¾Ø±Ø¯Ø§Ø²Ø´ ÙØ§ÛŒÙ„ Ø§Ú©Ø³Ù„ **'{original_filename}'** (Ø´ÛŒØª: {sheet_name_to_process}): Ø´ÛŒØª Ø¨Ø§ Ù†Ø§Ù… ÙˆØ§Ø±Ø¯ Ø´Ø¯Ù‡ Ù¾ÛŒØ¯Ø§ Ù†Ø´Ø¯. **Ù„Ø·ÙØ§Ù‹ Ù†Ø§Ù… Ø´ÛŒØª Ø±Ø§ Ø¨Ø±Ø±Ø³ÛŒ Ú©Ù†ÛŒØ¯.**")
            else:
                status.write(f"âŒ Ø®Ø·Ø§ÛŒ Ù¾Ø±Ø¯Ø§Ø²Ø´ ÙØ§ÛŒÙ„ Ø§Ú©Ø³Ù„ **'{original_filename}'** (Ø´ÛŒØª: {sheet_name_to_process}): Ø³Ø§Ø®ØªØ§Ø± Ø´ÛŒØª Ø§Ú©Ø³Ù„ Ù…Ø·Ø§Ø¨Ù‚Øª Ù†Ø¯Ø§Ø±Ø¯. (Ø¬Ø²Ø¦ÛŒØ§Øª Ø®Ø·Ø§: {e})")

        except Exception as e:
            status.write(f"âŒ Ø®Ø·Ø§ÛŒ Ù†Ø§Ù…Ø´Ø®Øµ Ù‡Ù†Ú¯Ø§Ù… Ù¾Ø±Ø¯Ø§Ø²Ø´ ÙØ§ÛŒÙ„ **'{original_filename}'**: {e}")

    # Final status update with the bug fix
    if success_count == total_files:
        status.update(label="âœ… ØªÙ…Ø§Ù… ÙØ§ÛŒÙ„â€ŒÙ‡Ø§ Ø¨Ø§ Ù…ÙˆÙÙ‚ÛŒØª Ù¾Ø±Ø¯Ø§Ø²Ø´ Ùˆ Ø¢Ù¾Ù„ÙˆØ¯ Ø´Ø¯Ù†Ø¯!", state="complete", expanded=False)
    else:
        status.update(label=f"âš ï¸ {success_count} Ø§Ø² {total_files} ÙØ§ÛŒÙ„ Ø¨Ø§ Ù…ÙˆÙÙ‚ÛŒØª Ù¾Ø±Ø¯Ø§Ø²Ø´ Ø´Ø¯Ù†Ø¯. Ø¬Ø²Ø¦ÛŒØ§Øª Ø±Ø§ Ø¨Ø±Ø±Ø³ÛŒ Ú©Ù†ÛŒØ¯.", state="error", expanded=True)
        
    st.cache_data.clear() # Clear all caches to force data reload from DB
    st.rerun() 
# ----------------------------------------------------------------------------------------------------------------

# --- Main Application Logic (Navigation) ---

# Manage page state with st.session_state
if 'page' not in st.session_state:
    st.session_state.page = "Data Analyzing Dashboard"  # Set default page to Dashboard

st.sidebar.header("Ù…Ù†ÙˆÛŒ Ø¨Ø±Ù†Ø§Ù…Ù‡")
# Contact Me page is removed from navigation
page_options = ["Data Analyzing Dashboard", "Upload Data", "Data Archive", "Trend Analysis"] 
selected_page_index = page_options.index(st.session_state.page)
selected_page = st.sidebar.radio("Ø¨Ø±Ùˆ Ø¨Ù‡:", options=page_options, index=selected_page_index, key="sidebar_radio")

# Update session state based on radio selection
if selected_page != st.session_state.page:
    st.session_state.page = selected_page
    st.rerun()  # Rerun to switch page immediately


if st.session_state.page == "Upload Data":
    st.header("â¬†ï¸ Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ ÙØ§ÛŒÙ„â€ŒÙ‡Ø§ÛŒ Ø§Ú©Ø³Ù„ Ùˆ Ø¯Ø±Ø¬ Ø¯Ø± Ù¾Ø§ÛŒÚ¯Ø§Ù‡ Ø¯Ø§Ø¯Ù‡")

    sheet_name_to_process = st.text_input(
        "Ù†Ø§Ù… Ø´ÛŒØª (Sheet Name) Ø­Ø§ÙˆÛŒ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§:",
        value="daily", # Default value to help the user
        help="Ù†Ø§Ù… Ø¯Ù‚ÛŒÙ‚ Ø´ÛŒØªÛŒ Ú©Ù‡ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ ØªÙˆÙ„ÛŒØ¯ Ùˆ Ø®Ø·Ø§ Ø¯Ø± Ø¢Ù† Ù‚Ø±Ø§Ø± Ø¯Ø§Ø±Ù†Ø¯ (Ø­Ø³Ø§Ø³ Ø¨Ù‡ Ø­Ø±ÙˆÙ Ú©ÙˆÚ†Ú© Ùˆ Ø¨Ø²Ø±Ú¯)."
    )

    uploaded_files = st.file_uploader(
        "ÙØ§ÛŒÙ„â€ŒÙ‡Ø§ÛŒ Ø§Ú©Ø³Ù„ (.xlsx) Ø±Ø§ Ø§ÛŒÙ†Ø¬Ø§ Ø¢Ù¾Ù„ÙˆØ¯ Ú©Ù†ÛŒØ¯", 
        type=["xlsx"], 
        accept_multiple_files=True
    )

    if st.button("ğŸš€ Ø´Ø±ÙˆØ¹ Ù¾Ø±Ø¯Ø§Ø²Ø´ Ùˆ Ø¯Ø±Ø¬ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§"):
        if not uploaded_files:
            st.warning("Ù„Ø·ÙØ§Ù‹ Ø§Ø¨ØªØ¯Ø§ ÙØ§ÛŒÙ„â€ŒÙ‡Ø§ Ø±Ø§ Ø§Ù†ØªØ®Ø§Ø¨ Ú©Ù†ÛŒØ¯.")
        elif not sheet_name_to_process.strip():
            st.error("Ù„Ø·ÙØ§Ù‹ Ù†Ø§Ù… Ø´ÛŒØª (Sheet Name) Ø±Ø§ ÙˆØ§Ø±Ø¯ Ú©Ù†ÛŒØ¯.")
        else:
            process_and_insert_data(uploaded_files, sheet_name_to_process.strip())


elif st.session_state.page == "Data Archive":
    # The original Data Archive page logic (for file storage management) is kept here.
    # ... (Original code for Data Archive: search, download, delete bucket) ...
    st.header("ğŸ“¦ Ø¢Ø±Ø´ÛŒÙˆ ÙØ§ÛŒÙ„â€ŒÙ‡Ø§ÛŒ Ø®Ø§Ù…")
    st.warning("Ø§ÛŒÙ† Ø¨Ø®Ø´ ØµØ±ÙØ§Ù‹ Ù…Ø¯ÛŒØ±ÛŒØª ÙØ§ÛŒÙ„â€ŒÙ‡Ø§ÛŒ Ø§Ú©Ø³Ù„ Ø®Ø§Ù… Ø¯Ø± Supabase Storage (Ø¨Ø®Ø´ Ø¢Ø±Ø´ÛŒÙˆ) Ø§Ø³ØªØŒ Ù†Ù‡ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ Ø¯Ø±ÙˆÙ† Ø¬Ø¯Ø§ÙˆÙ„ ØªØ­Ù„ÛŒÙ„.")
    # You must insert the original logic for Data Archive here from your file.


elif st.session_state.page == "Data Analyzing Dashboard":
    st.header("ğŸ“ˆ Ø¯Ø§Ø´Ø¨ÙˆØ±Ø¯ ØªØ­Ù„ÛŒÙ„ OEE Ùˆ ØªÙˆÙ„ÛŒØ¯")

    # Load all data from DB tables (faster and more reliable than re-parsing Excel)
    df_prod_all = load_data_from_supabase_tables(PROD_TABLE)
    df_err_all = load_data_from_supabase_tables(ERROR_TABLE)

    if df_prod_all.empty:
        st.warning("Ø¯Ø§Ø¯Ù‡â€ŒØ§ÛŒ Ø¨Ø±Ø§ÛŒ ØªØ­Ù„ÛŒÙ„ ÙˆØ¬ÙˆØ¯ Ù†Ø¯Ø§Ø±Ø¯. Ù„Ø·ÙØ§Ù‹ Ø§Ø¨ØªØ¯Ø§ Ø§Ø² Ø¨Ø®Ø´ 'Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ ÙØ§ÛŒÙ„â€ŒÙ‡Ø§'ØŒ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ Ø±Ø§ Ø¯Ø±Ø¬ Ú©Ù†ÛŒØ¯.")
        st.info(f"Ø§Ø·Ù„Ø§Ø¹Ø§Øª Ø§Ø² Ø¬Ø¯Ø§ÙˆÙ„ `{PROD_TABLE}` Ùˆ `{ERROR_TABLE}` Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ Ù…ÛŒâ€ŒØ´ÙˆØ¯.")
        st.markdown("---")
    else:
        # --- Filters ---
        col_filters, col_date = st.columns([1, 3])
        
        # 1. Date Filter
        min_available_date = df_prod_all['Date'].min()
        max_available_date = df_prod_all['Date'].max()
        
        with col_date:
            date_range = st.date_input(
                "Ø¨Ø§Ø²Ù‡ ØªØ§Ø±ÛŒØ®:",
                value=(min_available_date, max_available_date),
                min_value=min_available_date,
                max_value=max_available_date,
                key="dashboard_date_range"
            )
            if len(date_range) == 2:
                selected_start_date, selected_end_date = date_range
            else:
                st.warning("Ù„Ø·ÙØ§Ù‹ Ø¨Ø§Ø²Ù‡ ØªØ§Ø±ÛŒØ® Ø±Ø§ Ù…Ø´Ø®Øµ Ú©Ù†ÛŒØ¯.")
                selected_start_date, selected_end_date = min_available_date, max_available_date
        
        # Filter data by date
        df_prod_filtered = df_prod_all[
            (df_prod_all['Date'] >= selected_start_date) & 
            (df_prod_all['Date'] <= selected_end_date)
        ].copy()
        df_err_filtered = df_err_all[
            (df_err_all['Date'] >= selected_start_date) & 
            (df_err_all['Date'] <= selected_end_date)
        ].copy()

        # 2. Machine Filter
        unique_machines = ['All Machines'] + sorted(df_prod_filtered["ProductionTypeForTon"].unique().tolist())
        with col_filters:
            selected_machine = st.selectbox("Ø§Ù†ØªØ®Ø§Ø¨ Ù…Ø§Ø´ÛŒÙ†:", unique_machines)

        if selected_machine != 'All Machines':
            df_prod_filtered = df_prod_filtered[
                df_prod_filtered["ProductionTypeForTon"] == selected_machine
            ].copy()
            df_err_filtered = df_err_filtered[
                df_err_filtered["MachineType"] == selected_machine
            ].copy()

        # --- OEE Calculations ---
        oee_pct, line_efficiency_pct, availability_pct, performance_pct, quality_pct, \
            total_down_time_min, total_good_qty, total_pack_qty = calculate_oee_metrics(df_prod_filtered, df_err_filtered)

        # --- Display KPIs (Metrics) ---
        st.markdown("### Ø´Ø§Ø®Øµâ€ŒÙ‡Ø§ÛŒ Ø¹Ù…Ù„Ú©Ø±Ø¯ Ú©Ù„ÛŒØ¯ÛŒ (KPIs)")
        col1, col2, col3, col4, col5 = st.columns(5)
        
        # Format the metric display
        def display_metric(col, label, value, delta=None):
            if delta is not None:
                col.metric(label, f"{value:,.1f} %", delta=f"{delta:,.1f} %")
            else:
                col.metric(label, f"{value:,.1f} %")

        display_metric(col1, "OEE", oee_pct)
        display_metric(col2, "Ø¯Ø± Ø¯Ø³ØªØ±Ø³ Ø¨ÙˆØ¯Ù† (Availability)", availability_pct)
        display_metric(col3, "Ø¹Ù…Ù„Ú©Ø±Ø¯ (Performance)", performance_pct)
        display_metric(col4, "Ú©ÛŒÙÛŒØª (Quality)", quality_pct)
        display_metric(col5, "Ø¨Ø§Ø²Ø¯Ù‡ÛŒ Ø®Ø· (Line Efficiency)", line_efficiency_pct)

        col_prod, col_downtime = st.columns(2)
        
        with col_prod:
            col_prod.metric("Ø¨Ø³ØªÙ‡â€ŒØ¨Ù†Ø¯ÛŒ Ú©Ù„ (Units)", f"{total_pack_qty:,.0f} Ø¨Ø³ØªÙ‡")
        with col_downtime:
            col_downtime.metric("ØªÙˆÙ‚Ù Ú©Ù„ (Downtime)", f"{total_down_time_min:,.0f} Ø¯Ù‚ÛŒÙ‚Ù‡")
        
        st.markdown("---")

        # --- Charts ---
        
        # 1. OEE Component Breakdown (Gauge/Radial Chart)
        st.subheader("ØªØ­Ù„ÛŒÙ„ Ø§Ø¬Ø²Ø§ÛŒ OEE")
        
        # Using a Gauge chart for better visual
        fig_oee = go.Figure()
        
        # Availability
        fig_oee.add_trace(go.Indicator(
            mode="gauge+number",
            value=availability_pct,
            title={'text': "Availability"},
            gauge={'axis': {'range': [None, 100]},
                   'bar': {'color': "#2A8C8C"},
                   'steps': [{'range': [0, 60], 'color': "#FF5733"}, {'range': [60, 85], 'color': "#FFC300"}],
                   'threshold': {'line': {'color': "red", 'width': 4}, 'thickness': 0.75, 'value': 85}},
            domain={'row': 0, 'column': 0}
        ))
        
        # Performance
        fig_oee.add_trace(go.Indicator(
            mode="gauge+number",
            value=performance_pct,
            title={'text': "Performance"},
            gauge={'axis': {'range': [None, 100]},
                   'bar': {'color': "#00AEEF"},
                   'steps': [{'range': [0, 60], 'color': "#FF5733"}, {'range': [60, 85], 'color': "#FFC300"}],
                   'threshold': {'line': {'color': "red", 'width': 4}, 'thickness': 0.75, 'value': 85}},
            domain={'row': 0, 'column': 1}
        ))
        
        # Quality
        fig_oee.add_trace(go.Indicator(
            mode="gauge+number",
            value=quality_pct,
            title={'text': "Quality"},
            gauge={'axis': {'range': [None, 100]},
                   'bar': {'color': "#2ECC71"},
                   'steps': [{'range': [0, 90], 'color': "#FF5733"}, {'range': [90, 95], 'color': "#FFC300"}],
                   'threshold': {'line': {'color': "red", 'width': 4}, 'thickness': 0.75, 'value': 95}},
            domain={'row': 1, 'column': 0}
        ))
        
        # OEE
        fig_oee.add_trace(go.Indicator(
            mode="gauge+number",
            value=oee_pct,
            title={'text': "OEE"},
            gauge={'axis': {'range': [None, 100]},
                   'bar': {'color': "#8E44AD"},
                   'steps': [{'range': [0, 60], 'color': "#FF5733"}, {'range': [60, 85], 'color': "#FFC300"}],
                   'threshold': {'line': {'color': "red", 'width': 4}, 'thickness': 0.75, 'value': 85}},
            domain={'row': 1, 'column': 1}
        ))
        
        fig_oee.update_layout(
            grid={'rows': 2, 'columns': 2, 'pattern': "independent"},
            height=600,
            margin=dict(l=20, r=20, t=50, b=20)
        )
        st.plotly_chart(fig_oee, use_container_width=True)

        st.markdown("---")

        # 2. Top Downtime Reasons (Bar Chart)
        if not df_err_filtered.empty:
            st.subheader("Û±Û° Ù…ÙˆØ±Ø¯ Ø¨Ø±ØªØ± Ø¯Ù„Ø§ÛŒÙ„ ØªÙˆÙ‚Ù")
            
            # Aggregate total downtime by error
            top_errors = df_err_filtered.groupby("Error")["Duration"].sum().reset_index()
            top_errors = top_errors.sort_values(by="Duration", ascending=False).head(10)

            fig_err = px.bar(top_errors, x="Error", y="Duration",
                             title="Ø¯Ù„Ø§ÛŒÙ„ ØªÙˆÙ‚Ù (Ø¨Ø± Ø­Ø³Ø¨ Ø¯Ù‚ÛŒÙ‚Ù‡)",
                             labels={"Duration": "Ù…Ø¯Øª Ø²Ù…Ø§Ù† (Ø¯Ù‚ÛŒÙ‚Ù‡)", "Error": "Ø¯Ù„ÛŒÙ„ ØªÙˆÙ‚Ù"},
                             color="Duration",
                             color_continuous_scale=px.colors.sequential.Sunset)
            fig_err.update_traces(texttemplate='%{y:.1f}', textposition='outside')
            st.plotly_chart(fig_err, use_container_width=True)
        else:
            st.info("Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ ØªÙˆÙ‚Ù (Error) Ø¨Ø±Ø§ÛŒ Ø¨Ø§Ø²Ù‡ Ø§Ù†ØªØ®Ø§Ø¨ÛŒ ÙˆØ¬ÙˆØ¯ Ù†Ø¯Ø§Ø±Ø¯.")
            
        st.markdown("---")
        
        # 3. Production Tons by Product (Treemap - Kept from original code but enhanced)
        st.subheader("Ù…Ù‚Ø¯Ø§Ø± ØªÙˆÙ„ÛŒØ¯ (Ton) Ø¨Ø± Ø§Ø³Ø§Ø³ Ù…Ø­ØµÙˆÙ„")
        
        total_ton_per_product = df_prod_filtered.groupby("Product")["Ton"].sum().reset_index()
        total_ton_per_product = total_ton_per_product.sort_values(by="Ton", ascending=False)
        
        fig_ton = px.treemap(total_ton_per_product, path=[px.Constant("Ú©Ù„ Ù…Ø­ØµÙˆÙ„Ø§Øª"), 'Product'], values="Ton", 
                             title="ØªÙˆØ²ÛŒØ¹ ØªÙˆÙ„ÛŒØ¯ (Ton) Ø¨Ø± Ø§Ø³Ø§Ø³ Ù…Ø­ØµÙˆÙ„",
                             color="Ton", color_continuous_scale=px.colors.sequential.Teal)
        fig_ton.update_layout(margin=dict(t=50, l=25, r=25, b=25))
        st.plotly_chart(fig_ton, use_container_width=True)


elif st.session_state.page == "Trend Analysis":
    st.header("â³ ØªØ­Ù„ÛŒÙ„ Ø±ÙˆÙ†Ø¯ Ø²Ù…Ø§Ù†ÛŒ (Trend Analysis)")

    # Load all data from DB tables
    df_prod_all = load_data_from_supabase_tables(PROD_TABLE)
    df_err_all = load_data_from_supabase_tables(ERROR_TABLE)

    if df_prod_all.empty:
        st.warning("Ø¯Ø§Ø¯Ù‡â€ŒØ§ÛŒ Ø¨Ø±Ø§ÛŒ ØªØ­Ù„ÛŒÙ„ Ø±ÙˆÙ†Ø¯ ÙˆØ¬ÙˆØ¯ Ù†Ø¯Ø§Ø±Ø¯. Ù„Ø·ÙØ§Ù‹ Ø§Ø¨ØªØ¯Ø§ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ Ø±Ø§ Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ Ú©Ù†ÛŒØ¯.")
    else:
        # Aggregate data by Date for Trend Analysis
        daily_summary = df_prod_all.groupby("Date").agg(
            TotalPackQty=('PackQty', 'sum'),
            TotalWaste=('Waste', 'sum'),
            TotalDuration=('Duration', 'sum'), # in hours
            AvgCapacity=('Capacity', 'mean')
        ).reset_index()
        
        daily_errors = df_err_all.groupby("Date")["Duration"].sum().reset_index().rename(columns={"Duration": "TotalDowntime"}) # in minutes

        daily_df = pd.merge(daily_summary, daily_errors, on="Date", how="left").fillna(0)
        
        # Convert TotalDuration (hours) to minutes
        daily_df['TotalDurationMin'] = daily_df['TotalDuration'] * 60
        
        # Calculate OEE components for each day (reusing the logic in a lambda function or loop is too complex/slow)
        # We perform the component calculation on the already aggregated daily data for simplicity in trend
        
        daily_df['OperatingTime'] = daily_df['TotalDurationMin'] - daily_df['TotalDowntime']
        daily_df['OperatingTime'] = daily_df['OperatingTime'].apply(lambda x: max(0, x))
        
        # Availability
        daily_df['Availability'] = np.where(daily_df['TotalDurationMin'] > 0, 
                                            (daily_df['OperatingTime'] / daily_df['TotalDurationMin']) * 100, 0)
        
        # Quality
        daily_df['TotalGoodQty'] = daily_df['TotalPackQty'] - daily_df['TotalWaste']
        daily_df['Quality'] = np.where(daily_df['TotalPackQty'] > 0, 
                                       (daily_df['TotalGoodQty'] / daily_df['TotalPackQty']) * 100, 0)

        # Performance
        daily_df['IdealCycleRatePerMin'] = daily_df['AvgCapacity'] / 60
        daily_df['TheoreticalRunTime'] = np.where(daily_df['IdealCycleRatePerMin'] > 0, 
                                                 daily_df['TotalPackQty'] / daily_df['IdealCycleRatePerMin'], 0)
        daily_df['Performance'] = np.where(daily_df['OperatingTime'] > 0, 
                                            (daily_df['TheoreticalRunTime'] / daily_df['OperatingTime']) * 100, 0)
        daily_df['Performance'] = daily_df['Performance'].apply(lambda x: min(x, 100)) # Cap at 100%

        # OEE
        daily_df['OEE'] = (daily_df['Availability'] / 100) * (daily_df['Performance'] / 100) * (daily_df['Quality'] / 100) * 100
        
        
        # --- Display Charts ---

        st.subheader("Ø±ÙˆÙ†Ø¯ OEE Ùˆ Ø§Ø¬Ø²Ø§ÛŒ Ø¢Ù†")
        fig_trend = px.line(daily_df, x="Date", y=["OEE", "Availability", "Performance", "Quality"], 
                            title="Ø±ÙˆÙ†Ø¯ Ø±ÙˆØ²Ø§Ù†Ù‡ OEE Ùˆ Ø§Ø¬Ø²Ø§ÛŒ Ø¢Ù†",
                            labels={"value": "Ø¯Ø±ØµØ¯ (%)", "Date": "ØªØ§Ø±ÛŒØ®"},
                            template="plotly_white")
        fig_trend.update_layout(legend_title_text='Ø´Ø§Ø®Øµ')
        st.plotly_chart(fig_trend, use_container_width=True)

        st.subheader("Ø±ÙˆÙ†Ø¯ Ø±ÙˆØ²Ø§Ù†Ù‡ ØªÙˆÙ„ÛŒØ¯ (Ton) Ùˆ ØªÙˆÙ‚Ù (Downtime)")
        
        # Create a dual-axis chart for Ton and Downtime
        fig_dual = go.Figure()

        # Bar chart for Production (Tons)
        fig_dual.add_trace(go.Bar(
            x=daily_df['Date'],
            y=daily_df['TotalPackQty'],
            name='ØªÙˆÙ„ÛŒØ¯ Ú©Ù„ (Ø¨Ø³ØªÙ‡)',
            yaxis='y1',
            marker_color='skyblue'
        ))

        # Line chart for Downtime (Minutes)
        fig_dual.add_trace(go.Scatter(
            x=daily_df['Date'],
            y=daily_df['TotalDowntime'],
            name='ØªÙˆÙ‚Ù Ú©Ù„ (Ø¯Ù‚ÛŒÙ‚Ù‡)',
            yaxis='y2',
            mode='lines+markers',
            marker_color='red'
        ))

        fig_dual.update_layout(
            title='Ø±ÙˆÙ†Ø¯ Ø±ÙˆØ²Ø§Ù†Ù‡ ØªÙˆÙ„ÛŒØ¯ Ùˆ ØªÙˆÙ‚Ù',
            yaxis=dict(
                title='ØªÙˆÙ„ÛŒØ¯ Ú©Ù„ (Ø¨Ø³ØªÙ‡)',
                titlefont=dict(color='skyblue'),
                tickfont=dict(color='skyblue')
            ),
            yaxis2=dict(
                title='ØªÙˆÙ‚Ù Ú©Ù„ (Ø¯Ù‚ÛŒÙ‚Ù‡)',
                titlefont=dict(color='red'),
                tickfont=dict(color='red'),
                overlaying='y',
                side='right'
            ),
            legend=dict(x=0, y=1.1, orientation="h")
        )
        st.plotly_chart(fig_dual, use_container_width=True)

# NOTE: The remaining original helper functions are required for the code to run correctly
# and should be included by the user in the final app.py file.
