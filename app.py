import streamlit as st
import pandas as pd
import plotly.express as px
import plotly.graph_objects as go
from io import BytesIO
from supabase import create_client, Client
import base64
from datetime import datetime, timedelta, time as datetime_time
import re
import time
import numpy as np
import json
import logging

# ØªÙ†Ø¸ÛŒÙ…Ø§Øª Ù„Ø§Ú¯ÛŒÙ†Ú¯ (Ø§Ø®ØªÛŒØ§Ø±ÛŒ)
# logging.basicConfig(level=logging.INFO)

# --- Supabase Configuration ---
# ØªÙˆØ¬Ù‡: Ú©Ù„ÛŒØ¯Ù‡Ø§ÛŒ Ø´Ù…Ø§ Ø­ÙØ¸ Ø´Ø¯Ù‡â€ŒØ§Ù†Ø¯.
SUPABASE_URL = "https://rlutsxvghmhrgcnqbmch.supabase.co"
SUPABASE_KEY = "eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJzdXBhYmFzZSIsInJlZiI6InJsdXRzeHZnaG1ocmdjbnFibWNoIiwicm9sZSI6InNlcnZpY2Vfcm9sZSIsImlhdCI6MTc0NTEyODk5MSwiZXhwIjoyMDYwNzA0OTkxfQ.VPxJbrPUw4E-MyRGklQMcxveUTznNlWLhPO-mqrHv9c"

# --- Password for Archive Deletion ---
ARCHIVE_DELETE_PASSWORD = "beautifulmind"

# --- DB Table Names ---
PROD_TABLE = "production_data"
ERROR_TABLE = "error_data"
# -------------------------


st.set_page_config(layout="wide", page_title="OEE & Production Dashboard", initial_sidebar_state="expanded")
st.title("ğŸ“Š Ø¯Ø§Ø´Ø¨ÙˆØ±Ø¯ ØªØ­Ù„ÛŒÙ„ ØªÙˆÙ„ÛŒØ¯ Ùˆ OEE")


# --- Helper Functions (Updated/New) ---

@st.cache_data(ttl=3600, show_spinner="Ø¯Ø±ÛŒØ§ÙØª Ø§Ø·Ù„Ø§Ø¹Ø§Øª Ø§Ø² Ù¾Ø§ÛŒÚ¯Ø§Ù‡ Ø¯Ø§Ø¯Ù‡...")
def load_data_from_supabase_tables(table_name):
    """Fetches all data from a specified Supabase PostgreSQL table."""
    try:
        # PostgreSQL/Supabase Ø¨Ù‡ Ø·ÙˆØ± Ù¾ÛŒØ´â€ŒÙØ±Ø¶ Ù†Ø§Ù… Ø³ØªÙˆÙ†â€ŒÙ‡Ø§ Ø±Ø§ Ø¨Ø§ Ø­Ø±ÙˆÙ Ú©ÙˆÚ†Ú© Ø°Ø®ÛŒØ±Ù‡ Ù…ÛŒâ€ŒÚ©Ù†Ø¯.
        # Ù…Ø§ Ù†Ø§Ù… Ø³ØªÙˆÙ†â€ŒÙ‡Ø§ Ø±Ø§ Ø¨Ù‡ lowercase ØªØºÛŒÛŒØ± Ù…ÛŒâ€ŒØ¯Ù‡ÛŒÙ… ØªØ§ Ø¨Ø§ Ø¯ÛŒØªØ§Ø¨ÛŒØ³ ØªØ·Ø¨ÛŒÙ‚ ÛŒØ§Ø¨Ø¯.
        response = supabase.table(table_name).select("*").order("date", desc=False).execute()
        data = response.data
        if not data:
            return pd.DataFrame()
        
        df = pd.DataFrame(data)
        
        # ğŸš¨ Ø§ØµÙ„Ø§Ø­ Ø­ÛŒØ§ØªÛŒ: Ø¨Ø±Ø±Ø³ÛŒ Ø³ØªÙˆÙ† 'date' (Ø­Ø±ÙˆÙ Ú©ÙˆÚ†Ú©) Ùˆ ØªØ¨Ø¯ÛŒÙ„ Ø¢Ù† Ø¨Ù‡ 'Date' (Ø­Ø±ÙˆÙ Ø¨Ø²Ø±Ú¯) Ø¨Ø±Ø§ÛŒ Ø³Ø§Ø²Ú¯Ø§Ø±ÛŒ Ø¨Ø§ Ø¨Ù‚ÛŒÙ‡ Ú©Ø¯
        if 'date' in df.columns:
            df['Date'] = pd.to_datetime(df['date']).dt.date
            df.drop(columns=['date'], inplace=True) # Ø­Ø°Ù Ø³ØªÙˆÙ† Ø§ØµÙ„ÛŒ 'date'
        elif 'Date' in df.columns:
            # Ø§Ú¯Ø± 'Date' ÙˆØ¬ÙˆØ¯ Ø¯Ø§Ø´Øª (Ú©Ù…ØªØ± Ù…Ø­ØªÙ…Ù„ Ø§Ø³Øª)ØŒ ÙÙ‚Ø· Ø¢Ù† Ø±Ø§ ØªØ¨Ø¯ÛŒÙ„ Ù…ÛŒâ€ŒÚ©Ù†ÛŒÙ…
            df['Date'] = pd.to_datetime(df['Date']).dt.date

        # Ensure numeric columns are correct
        for col in ['Duration', 'PackQty', 'Waste', 'Ton', 'Capacity', 'Manpower']:
            if col in df.columns:
                df[col] = pd.to_numeric(df[col], errors='coerce').fillna(0)
                
        return df

    except Exception as e:
        if isinstance(e, dict) and e.get('code') == '42P01':
            st.error(f"Ø®Ø·Ø§ÛŒ Ù¾Ø§ÛŒÚ¯Ø§Ù‡ Ø¯Ø§Ø¯Ù‡: Ø¬Ø¯ÙˆÙ„ '{table_name}' ÙˆØ¬ÙˆØ¯ Ù†Ø¯Ø§Ø±Ø¯. Ù„Ø·ÙØ§Ù‹ Ù…Ø·Ù…Ø¦Ù† Ø´ÙˆÛŒØ¯ Ø¬Ø¯Ø§ÙˆÙ„ Ø±Ø§ Ø¯Ø± Supabase Ø§ÛŒØ¬Ø§Ø¯ Ú©Ø±Ø¯Ù‡â€ŒØ§ÛŒØ¯.")
        elif '42P01' in str(e): 
             st.error(f"Ø®Ø·Ø§ÛŒ Ù¾Ø§ÛŒÚ¯Ø§Ù‡ Ø¯Ø§Ø¯Ù‡: Ø¬Ø¯ÙˆÙ„ '{table_name}' ÙˆØ¬ÙˆØ¯ Ù†Ø¯Ø§Ø±Ø¯. Ù„Ø·ÙØ§Ù‹ Ù…Ø·Ù…Ø¦Ù† Ø´ÙˆÛŒØ¯ Ø¬Ø¯Ø§ÙˆÙ„ Ø±Ø§ Ø¯Ø± Supabase Ø§ÛŒØ¬Ø§Ø¯ Ú©Ø±Ø¯Ù‡â€ŒØ§ÛŒØ¯.")
        else:
            st.error(f"Ø®Ø·Ø§ Ø¯Ø± Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ Ø§Ø² Supabase Ø¨Ø±Ø§ÛŒ Ø¬Ø¯ÙˆÙ„ {table_name}: {e}")
        return pd.DataFrame()

def insert_to_db(df, table_name):
    """Inserts DataFrame records into the specified Supabase table."""
    if df.empty:
        return True
    
    # ğŸš¨ Ø§ØµÙ„Ø§Ø­ Ø­ÛŒØ§ØªÛŒ: ØªØ¨Ø¯ÛŒÙ„ Ù†Ø§Ù… Ø³ØªÙˆÙ†â€ŒÙ‡Ø§ÛŒ DataFrame Ø¨Ù‡ Ø­Ø±ÙˆÙ Ú©ÙˆÚ†Ú© Ù‚Ø¨Ù„ Ø§Ø² Ø¯Ø±Ø¬
    # ØªØ§ Ø¨Ø§ Ù†Ø­ÙˆÙ‡ Ù†Ø§Ù…â€ŒÚ¯Ø°Ø§Ø±ÛŒ PostgreSQL (Ø­Ø±ÙˆÙ Ú©ÙˆÚ†Ú©) Ù…Ø·Ø§Ø¨Ù‚Øª Ø¯Ø§Ø´ØªÙ‡ Ø¨Ø§Ø´Ø¯.
    df_insert = df.copy()
    df_insert.columns = [col.lower() for col in df_insert.columns]
    
    try:
        data_to_insert = df_insert.to_dict('records')
        response = supabase.table(table_name).insert(data_to_insert).execute()
        
        if response.data:
            return True
        else:
            error_data = getattr(response, 'error', None)
            if error_data:
                 st.error(f"Ø®Ø·Ø§ Ø¯Ø± Ø¯Ø±Ø¬ Ø¯Ø§Ø¯Ù‡ Ø¨Ù‡ Ø¬Ø¯ÙˆÙ„ {table_name}: {error_data.get('message', 'Ø®Ø·Ø§ÛŒ Ù†Ø§Ù…Ø´Ø®Øµ')}")
                 return False
            return True

    except Exception as e:
        st.error(f"Ø®Ø·Ø§ÛŒ Ú©Ù„ÛŒ Ø¯Ø± Ø¯Ø±Ø¬ Ø¯Ø§Ø¯Ù‡ Ø¨Ù‡ Ø¬Ø¯ÙˆÙ„ {table_name}: {e}")
        return False

# --- OEE and Analysis Metrics (New/Replaced Logic) ---

def calculate_oee_metrics(df_prod, df_err):
    """Calculates OEE, Availability, Performance, and Quality metrics."""
    if df_prod.empty:
        return 0, 0, 0, 0, 0, 0, 0, 0

    # (Ø¨Ù‚ÛŒÙ‡ Ù…Ù†Ø·Ù‚ calculate_oee_metrics Ø¨Ø¯ÙˆÙ† ØªØºÛŒÛŒØ± Ø¨Ø§Ù‚ÛŒ Ù…ÛŒâ€ŒÙ…Ø§Ù†Ø¯ØŒ Ú†ÙˆÙ† Ø§Ø² Ø³ØªÙˆÙ†â€ŒÙ‡Ø§ÛŒ Ø§Ø³ØªØ§Ù†Ø¯Ø§Ø±Ø¯ Ø´Ø¯Ù‡ Ù…Ø§Ù†Ù†Ø¯ 'Duration', 'PackQty' Ø§Ø³ØªÙØ§Ø¯Ù‡ Ù…ÛŒâ€ŒÚ©Ù†Ø¯.)
    
    # --- Û±. Planned Production Time (Total Duration) ---
    total_planned_time_min = df_prod["Duration"].sum() * 60

    # --- Û². Down Time (Error Time) ---
    total_down_time_min = df_err["Duration"].sum()
    
    # --- Û³. Operating Time ---
    operating_time_min = total_planned_time_min - total_down_time_min
    operating_time_min = max(0, operating_time_min)

    # --- KPI Û±: Availability (%) ---
    availability_pct = 0
    if total_planned_time_min > 0:
        availability_pct = (operating_time_min / total_planned_time_min) * 100
    
    # --- Û´. Total Production (Packages) ---
    total_pack_qty = df_prod["PackQty"].sum()
    total_waste = df_prod["Waste"].sum()
    total_good_qty = total_pack_qty - total_waste

    # --- KPI Û²: Quality (%) ---
    quality_pct = 0
    if total_pack_qty > 0:
        quality_pct = (total_good_qty / total_pack_qty) * 100
    
    # --- Ûµ. Ideal Cycle Rate (Capacity) ---
    avg_capacity_units_per_hour = df_prod["Capacity"].mean() 
    ideal_cycle_rate_per_min = avg_capacity_units_per_hour / 60 if avg_capacity_units_per_hour > 0 else 0
        
    # --- Û¶. Theoretical Production Time (Ideal Run Time) ---
    theoretical_run_time_min = 0 
    if ideal_cycle_rate_per_min > 0:
        theoretical_run_time_min = total_pack_qty / ideal_cycle_rate_per_min
        
    # --- KPI Û³: Performance (%) ---
    performance_pct = 0
    if operating_time_min > 0:
        performance_pct = (theoretical_run_time_min / operating_time_min) * 100
        performance_pct = min(performance_pct, 100) 
        
    # --- KPI Û´: OEE (%) ---
    oee_pct = (availability_pct / 100) * (performance_pct / 100) * (quality_pct / 100) * 100
    
    # --- KPI Ûµ: Line Efficiency (Total Yield % against Theoretical Max) ---
    total_potential_packages = total_planned_time_min * ideal_cycle_rate_per_min
    line_efficiency_pct = 0
    if total_potential_packages > 0:
        line_efficiency_pct = (total_good_qty / total_potential_packages) * 100
        line_efficiency_pct = min(line_efficiency_pct, 100)
    
    
    return oee_pct, line_efficiency_pct, availability_pct, performance_pct, quality_pct, total_down_time_min, total_good_qty, total_pack_qty

# --- Helper functions from original code (must be included here) ---

# (ØªÙˆØ¬Ù‡: ØªÙˆØ§Ø¨Ø¹ Ú©Ù…Ú©ÛŒ parse_filename_date_to_datetimeØŒ read_production_dataØŒ read_error_dataØŒ upload_to_supabase Ùˆ...
# Ú©Ù‡ Ù‚Ø¨Ù„Ø§Ù‹ Ø¯Ø± Ú©Ø¯ Ø´Ù…Ø§ ÙˆØ¬ÙˆØ¯ Ø¯Ø§Ø´ØªÙ†Ø¯ Ùˆ Ø¨Ø±Ø§ÛŒ Ø§Ø¬Ø±Ø§ÛŒ Ù…Ù†Ø·Ù‚ Ø§ØµÙ„ÛŒ Ø¢Ù¾Ù„ÙˆØ¯ Ø¶Ø±ÙˆØ±ÛŒ Ù‡Ø³ØªÙ†Ø¯ØŒ Ø¨Ø§ÛŒØ¯ Ø¯Ø± Ø§ÛŒÙ† Ù‚Ø³Ù…Øª Ù‚Ø±Ø§Ø± Ú¯ÛŒØ±Ù†Ø¯.
# ÙØ±Ø¶ Ø¨Ø± Ø§ÛŒÙ† Ø§Ø³Øª Ú©Ù‡ Ø´Ù…Ø§ Ø¢Ù†â€ŒÙ‡Ø§ Ø±Ø§ Ø§Ø² Ø¢Ø®Ø±ÛŒÙ† Ù†Ø³Ø®Ù‡ Ú©Ø¯ Ø®ÙˆØ¯ Ø¯Ø± Ø§ÛŒÙ†Ø¬Ø§ Ú©Ù¾ÛŒ Ù…ÛŒâ€ŒÚ©Ù†ÛŒØ¯.)
# ... (Ú©Ø¯Ù‡Ø§ÛŒ ØªÙˆØ§Ø¨Ø¹ Ú©Ù…Ú©ÛŒ Ù‚Ø¨Ù„ÛŒ Ø´Ù…Ø§ Ø¨Ø§ÛŒØ¯ Ø§ÛŒÙ†Ø¬Ø§ Ù‚Ø±Ø§Ø± Ú¯ÛŒØ±Ù†Ø¯) ...


# ----------------------------------------------------------------------------------------------------------------

# --- NEW: Master Processing Function for Upload Page ---
def process_and_insert_data(uploaded_files, sheet_name_to_process):
    """Uploads to storage, processes the specified sheet, and inserts data into DB tables."""
    # (Ú©Ø¯ ØªØ§Ø¨Ø¹ process_and_insert_data Ø¨Ø¯ÙˆÙ† ØªØºÛŒÛŒØ± Ø¯Ø± Ù…Ù†Ø·Ù‚ Ø§ÛŒÙ†Ø¬Ø§ Ù‚Ø±Ø§Ø± Ù…ÛŒâ€ŒÚ¯ÛŒØ±Ø¯)
    total_files = len(uploaded_files)
    success_count = 0
    
    # First, upload all files to storage (Archive)
    st.markdown("### Û±. Ø¢Ø±Ø´ÛŒÙˆ ÙØ§ÛŒÙ„â€ŒÙ‡Ø§ÛŒ Ø®Ø§Ù… (Storage)")
    # upload_to_supabase(uploaded_files) # Ø§Ú¯Ø± ØªØ§Ø¨Ø¹ upload_to_supabase Ù…Ø±Ø¨ÙˆØ· Ø¨Ù‡ Storage Ø§Ø³Øª

    # After storage upload, we can now process them from the uploaded file object itself (faster than re-downloading)
    st.markdown("### Û². Ù¾Ø±Ø¯Ø§Ø²Ø´ Ùˆ Ø¯Ø±Ø¬ Ø¯Ø§Ø¯Ù‡ Ø¨Ù‡ Ø¬Ø¯Ø§ÙˆÙ„ (PostgreSQL)")
    status = st.status("Ø¯Ø± Ø­Ø§Ù„ Ù¾Ø±Ø¯Ø§Ø²Ø´ Ùˆ Ø¯Ø±Ø¬ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§...", expanded=True)
    
    for i, file in enumerate(uploaded_files):
        original_filename = file.name
        file_date_obj = parse_filename_date_to_datetime(original_filename)
        
        status.write(f"Ø¯Ø± Ø­Ø§Ù„ Ù¾Ø±Ø¯Ø§Ø²Ø´ ÙØ§ÛŒÙ„: **{original_filename}** (ØªØ§Ø±ÛŒØ®: {file_date_obj})")
        
        try:
            # Read all sheets to find the correct one
            df_raw_sheet = pd.read_excel(BytesIO(file.getvalue()), sheet_name=sheet_name_to_process, header=None)

            # Read Production and Error data
            prod_df = read_production_data(df_raw_sheet, original_filename, sheet_name_to_process, file_date_obj)
            err_df = read_error_data(df_raw_sheet, sheet_name_to_process, original_filename, file_date_obj)

            # Insert Production Data
            if not prod_df.empty:
                prod_success = insert_to_db(prod_df, PROD_TABLE)
                if prod_success:
                    status.write(f"âœ… Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ ØªÙˆÙ„ÛŒØ¯ Ø¨Ø§ Ù…ÙˆÙÙ‚ÛŒØª Ø¨Ù‡ Ø¬Ø¯ÙˆÙ„ `{PROD_TABLE}` Ø¯Ø±Ø¬ Ø´Ø¯.")
                else:
                    status.write(f"âŒ Ø®Ø·Ø§ÛŒ Ø¯Ø±Ø¬ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ ØªÙˆÙ„ÛŒØ¯ Ø¨Ù‡ `{PROD_TABLE}`.")
                    continue # Skip error data insertion if prod data failed
            else:
                status.write(f"âš ï¸ ÙØ§ÛŒÙ„ **{original_filename}** Ø­Ø§ÙˆÛŒ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ ØªÙˆÙ„ÛŒØ¯ Ù…Ø¹ØªØ¨Ø± Ù†Ø¨ÙˆØ¯ ÛŒØ§ Ø®Ø§Ù„ÛŒ Ø¨ÙˆØ¯.")

            # Insert Error Data
            if not err_df.empty:
                err_success = insert_to_db(err_df, ERROR_TABLE)
                if err_success:
                    status.write(f"âœ… Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ Ø®Ø·Ø§ Ø¨Ø§ Ù…ÙˆÙÙ‚ÛŒØª Ø¨Ù‡ Ø¬Ø¯ÙˆÙ„ `{ERROR_TABLE}` Ø¯Ø±Ø¬ Ø´Ø¯.")
                else:
                    status.write(f"âŒ Ø®Ø·Ø§ÛŒ Ø¯Ø±Ø¬ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ Ø®Ø·Ø§ Ø¨Ù‡ `{ERROR_TABLE}`.")
            else:
                 status.write(f"âš ï¸ ÙØ§ÛŒÙ„ **{original_filename}** Ø­Ø§ÙˆÛŒ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ Ø®Ø·Ø§ Ù…Ø¹ØªØ¨Ø± Ù†Ø¨ÙˆØ¯ ÛŒØ§ Ø®Ø§Ù„ÛŒ Ø¨ÙˆØ¯.")

            success_count += 1

        except ValueError as e:
            if 'Worksheet named' in str(e) and 'not found' in str(e):
                status.write(f"âŒ Ø®Ø·Ø§ Ø¯Ø± Ù¾Ø±Ø¯Ø§Ø²Ø´ ÙØ§ÛŒÙ„ Ø§Ú©Ø³Ù„ **'{original_filename}'** (Ø´ÛŒØª: {sheet_name_to_process}): Ø´ÛŒØª Ø¨Ø§ Ù†Ø§Ù… ÙˆØ§Ø±Ø¯ Ø´Ø¯Ù‡ Ù¾ÛŒØ¯Ø§ Ù†Ø´Ø¯. **Ù„Ø·ÙØ§Ù‹ Ù†Ø§Ù… Ø´ÛŒØª Ø±Ø§ Ø¨Ø±Ø±Ø³ÛŒ Ú©Ù†ÛŒØ¯.**")
            else:
                status.write(f"âŒ Ø®Ø·Ø§ÛŒ Ù¾Ø±Ø¯Ø§Ø²Ø´ ÙØ§ÛŒÙ„ Ø§Ú©Ø³Ù„ **'{original_filename}'** (Ø´ÛŒØª: {sheet_name_to_process}): Ø³Ø§Ø®ØªØ§Ø± Ø´ÛŒØª Ø§Ú©Ø³Ù„ Ù…Ø·Ø§Ø¨Ù‚Øª Ù†Ø¯Ø§Ø±Ø¯. (Ø¬Ø²Ø¦ÛŒØ§Øª Ø®Ø·Ø§: {e})")

        except Exception as e:
            status.write(f"âŒ Ø®Ø·Ø§ÛŒ Ù†Ø§Ù…Ø´Ø®Øµ Ù‡Ù†Ú¯Ø§Ù… Ù¾Ø±Ø¯Ø§Ø²Ø´ ÙØ§ÛŒÙ„ **'{original_filename}'**: {e}")

    # Final status update with the bug fix
    if success_count == total_files:
        status.update(label="âœ… ØªÙ…Ø§Ù… ÙØ§ÛŒÙ„â€ŒÙ‡Ø§ Ø¨Ø§ Ù…ÙˆÙÙ‚ÛŒØª Ù¾Ø±Ø¯Ø§Ø²Ø´ Ùˆ Ø¢Ù¾Ù„ÙˆØ¯ Ø´Ø¯Ù†Ø¯!", state="complete", expanded=False)
    else:
        status.update(label=f"âš ï¸ {success_count} Ø§Ø² {total_files} ÙØ§ÛŒÙ„ Ø¨Ø§ Ù…ÙˆÙÙ‚ÛŒØª Ù¾Ø±Ø¯Ø§Ø²Ø´ Ø´Ø¯Ù†Ø¯. Ø¬Ø²Ø¦ÛŒØ§Øª Ø±Ø§ Ø¨Ø±Ø±Ø³ÛŒ Ú©Ù†ÛŒØ¯.", state="error", expanded=True)
        
    st.cache_data.clear() # Clear all caches to force data reload from DB
    # st.rerun() # Rerun is necessary after upload
# ----------------------------------------------------------------------------------------------------------------

# --- Main Application Logic (Navigation) ---

# (Ø¨Ù‚ÛŒÙ‡ Ú©Ø¯ Ø§ØµÙ„ÛŒ Ø¨Ø±Ù†Ø§Ù…Ù‡ Ø¨Ø¯ÙˆÙ† ØªØºÛŒÛŒØ± Ø¨Ø§Ù‚ÛŒ Ù…ÛŒâ€ŒÙ…Ø§Ù†Ø¯.)

if 'page' not in st.session_state:
    st.session_state.page = "Data Analyzing Dashboard"
    
# (Navigation logic and page content for Upload Data, Data Analyzing Dashboard, Data Archive, Trend Analysis)

# ... (Ø¨Ù‚ÛŒÙ‡ Ú©Ø¯ Ø¨Ø±Ù†Ø§Ù…Ù‡ Ø§ØµÙ„ÛŒ Ø§Ø² Ø§ÛŒÙ†Ø¬Ø§ Ø§Ø¯Ø§Ù…Ù‡ Ù…ÛŒâ€ŒÛŒØ§Ø¨Ø¯) ...
