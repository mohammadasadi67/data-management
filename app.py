import streamlit as st
import pandas as pd
import plotly.express as px
import plotly.graph_objects as go
from io import BytesIO
from supabase import create_client, Client
import base64
from datetime import datetime, timedelta
import re
import numpy as np
import logging
import time

# ØªÙ†Ø¸ÛŒÙ…Ø§Øª Ø§ØµÙ„ÛŒ Ø¨Ø±Ù†Ø§Ù…Ù‡
st.set_page_config(layout="wide", page_title="OEE & Production Data Analytics System", initial_sidebar_state="expanded")
st.title("ğŸ§  Ø³ÛŒØ³ØªÙ… ØªØ­Ù„ÛŒÙ„ Ù‡ÙˆØ´Ù…Ù†Ø¯ Ø¹Ù…Ù„Ú©Ø±Ø¯ ØªÙˆÙ„ÛŒØ¯ (OEE)")

# ------------------------------------------------------------------------------
# --- Û±. ØªÙ†Ø¸ÛŒÙ…Ø§Øª Ø§ØªØµØ§Ù„ Ùˆ Ù…ØªØºÛŒØ±Ù‡Ø§ÛŒ Ø³Ø±Ø§Ø³Ø±ÛŒ ---
# ------------------------------------------------------------------------------

# --- Supabase Configuration (ØªØ§ÛŒÛŒØ¯ Ø´Ø¯Ù‡) ---
SUPABASE_URL = "https://rlutsxvghmhrgcnqbmch.supabase.co"
SUPABASE_KEY = "eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJzdXBhYmFzZSIsInJlZiI6InJsdXRzeHZnaG1ocmdjbnFibWNoIiwicm9sZSI6InNlcnZpY2Vfcm9sZSIsImlhdCI6MTc0NTEyODk5MSwiZXhwIjoyMDYwNzA0OTkxfQ.VPxJbrPUw4E-MyRGklQMcxveUTznNlWLhPO-mqrHv9c"

# --- DB Table Names ---
PROD_TABLE = "production_data"
ERROR_TABLE = "error_data"

# --- Password for Archive Deletion ---
ARCHIVE_DELETE_PASSWORD = "beautifulmind"

# Ø§ØªØµØ§Ù„ Ø¨Ù‡ Supabase (Cache Resource)
@st.cache_resource
def get_supabase_client():
    """Ø§ÛŒØ¬Ø§Ø¯ Ùˆ Ú©Ø´ Ú©Ø±Ø¯Ù† Ø§ØªØµØ§Ù„ Ø¨Ù‡ Supabase."""
    try:
        supabase: Client = create_client(SUPABASE_URL, SUPABASE_KEY)
        return supabase
    except Exception as e:
        st.error(f"Ø®Ø·Ø§ Ø¯Ø± Ø§ØªØµØ§Ù„ Ø¨Ù‡ Ù¾Ø§ÛŒÚ¯Ø§Ù‡ Ø¯Ø§Ø¯Ù‡ Supabase: {e}")
        st.stop() 

supabase = get_supabase_client()

# ------------------------------------------------------------------------------
# --- Û². ØªÙˆØ§Ø¨Ø¹ Ú©Ù…Ú©ÛŒ Ø§ØµÙ„ÛŒ Ùˆ Ù…Ø­Ø§Ø³Ø¨Ø§Øª (Ø¨Ø¯ÙˆÙ† ØªØºÛŒÛŒØ± Ù…Ù†Ø·Ù‚) ---
# ------------------------------------------------------------------------------

COLUMN_MAP = {
    'Ù…Ù‚Ø¯Ø§Ø± Ú©Ù„': 'PackQty', 
    'Ø¨Ø³ØªÙ‡': 'PackQty',
    'Ø¶Ø§ÛŒØ¹Ø§Øª': 'Waste',
    'ØªÙ†Ø§Ú˜': 'Ton',
    'Ø²Ù…Ø§Ù† ÙØ¹Ø§Ù„ÛŒØª': 'Duration',
    'Ø¸Ø±ÙÛŒØª': 'Capacity',
    'ØªØ¹Ø¯Ø§Ø¯ Ù†ÙØ±Ø§Øª': 'Manpower',
    'Ù…Ø¯Øª Ø²Ù…Ø§Ù†': 'Duration', 
}

def parse_filename_date_to_datetime(filename):
    match = re.search(r'(\d{8})', filename)
    if match:
        try:
            return datetime.strptime(match.group(1), '%d%m%Y').date()
        except ValueError:
            return None
    return None

def standardize_dataframe_for_oee(df, type='prod'):
    df_standard = df.rename(columns=lambda x: COLUMN_MAP.get(x.strip(), x.strip())).copy()
    
    required_cols_prod = ['PackQty', 'Waste', 'Duration', 'Capacity', 'Ton']
    required_cols_error = ['Duration']
    
    if type == 'prod':
        for col in required_cols_prod:
            if col in df_standard.columns:
                df_standard[col] = pd.to_numeric(df_standard[col], errors='coerce').fillna(0)
    elif type == 'error':
        for col in required_cols_error:
            if col in df_standard.columns:
                df_standard[col] = pd.to_numeric(df_standard[col], errors='coerce').fillna(0)
                
    return df_standard

def read_production_data(df_raw_sheet, uploaded_file_name, sheet_name_for_debug, file_date_obj):
    try:
        data_prod = df_raw_sheet.iloc[3:9, 3:16].copy() 
        headers_prod = df_raw_sheet.iloc[2, 3:16].tolist()
        data_prod.columns = headers_prod
        
        product_names = df_raw_sheet.iloc[3:9, 2].tolist() 
        
        data_prod['ProductTypeForTon'] = [str(p).strip() for p in product_names]
        df_prod = data_prod.melt(id_vars=['ProductTypeForTon'], var_name='MetricName', value_name='MetricValue').dropna(subset=['MetricValue'])
        
        df_prod = df_prod.pivot_table(index='ProductTypeForTon', columns='MetricName', values='MetricValue', aggfunc='first').reset_index()
        df_prod.columns.name = None
        
        df_prod = standardize_dataframe_for_oee(df_prod, type='prod')
        
        shift = df_raw_sheet.iloc[1, 1].strip() if not pd.isna(df_raw_sheet.iloc[1, 1]) else 'Unknown'
        
        df_prod['Date'] = file_date_obj
        df_prod['Shift'] = shift
        df_prod['Filename'] = uploaded_file_name
        
        return df_prod[['Date', 'Shift', 'Filename', 'ProductTypeForTon', 'PackQty', 'Waste', 'Duration', 'Capacity', 'Ton']]
        
    except Exception as e:
        return pd.DataFrame()


def read_error_data(df_raw_sheet, sheet_name_for_debug, uploaded_file_name_for_debug, file_date_obj):
    try:
        data_err = df_raw_sheet.iloc[12:15, 3:16].copy()
        headers_err = df_raw_sheet.iloc[11, 3:16].tolist()
        data_err.columns = headers_err
        
        machine_names = df_raw_sheet.iloc[12:15, 2].tolist() 
        
        data_err['MachineType'] = [str(m).strip() for m in machine_names]
        df_err = data_err.melt(id_vars=['MachineType'], var_name='Error', value_name='Duration').dropna(subset=['Duration'])
        
        df_err = standardize_dataframe_for_oee(df_err, type='error')
        
        shift = df_raw_sheet.iloc[1, 1].strip() if not pd.isna(df_raw_sheet.iloc[1, 1]) else 'Unknown'

        df_err['Date'] = file_date_obj
        df_err['Shift'] = shift
        df_err['Filename'] = uploaded_file_name_for_debug
        
        return df_err[['Date', 'Shift', 'Filename', 'MachineType', 'Error', 'Duration']]
        
    except Exception as e:
        return pd.DataFrame()


def upload_to_supabase(uploaded_files, bucket_name="production-archive"):
    try:
        for file in uploaded_files:
            file_path = f"{file.name}"
            supabase.storage.from_(bucket_name).upload(file_path, file.getvalue(), file_options={"content-type": file.type, "upsert": True})
        st.success(f"âœ… {len(uploaded_files)} ÙØ§ÛŒÙ„ Ø¨Ø§ Ù…ÙˆÙÙ‚ÛŒØª Ø¨Ù‡ Ø¢Ø±Ø´ÛŒÙˆ Ø°Ø®ÛŒØ±Ù‡ Ø´Ø¯Ù†Ø¯.")
        return True
    except Exception as e:
        st.error(f"âŒ Ø®Ø·Ø§ Ø¯Ø± Ø¢Ù¾Ù„ÙˆØ¯ ÙØ§ÛŒÙ„â€ŒÙ‡Ø§ Ø¨Ù‡ Supabase Storage: {e}")
        return False

@st.cache_data(ttl=3600, show_spinner="Ø¯Ø±ÛŒØ§ÙØª Ø§Ø·Ù„Ø§Ø¹Ø§Øª Ø§Ø² Ù¾Ø§ÛŒÚ¯Ø§Ù‡ Ø¯Ø§Ø¯Ù‡...")
def load_data_from_supabase_tables(table_name):
    try:
        response = supabase.table(table_name).select("*").execute()
        data = response.data
        if not data:
            return pd.DataFrame()
        
        df = pd.DataFrame(data)
        
        date_col_in_db = None
        if 'date' in df.columns:
            date_col_in_db = 'date'
        elif 'Date' in df.columns: 
            date_col_in_db = 'Date'
            
        if date_col_in_db:
            df['Date'] = pd.to_datetime(df[date_col_in_db]).dt.date
            if date_col_in_db == 'date': 
                df.drop(columns=['date'], inplace=True, errors='ignore')
            
        if 'Date' in df.columns:
            df = df.sort_values(by='Date', ascending=True).reset_index(drop=True)

        for col in ['Duration', 'PackQty', 'Waste', 'Ton', 'Capacity', 'Manpower']:
            col_lower = col.lower()
            if col_lower in df.columns: 
                df[col] = pd.to_numeric(df[col_lower], errors='coerce').fillna(0)
            elif col in df.columns:
                df[col] = pd.to_numeric(df[col], errors='coerce').fillna(0)
        
        return df

    except Exception as e:
        return pd.DataFrame()

def insert_to_db(df, table_name):
    if df.empty:
        return True
    
    df_insert = df.copy()
    df_insert.columns = [col.lower() for col in df_insert.columns]
    
    try:
        data_to_insert = df_insert.to_dict('records')
        response = supabase.table(table_name).insert(data_to_insert).execute()
        
        if response.data:
            return True
        else:
            error_data = getattr(response, 'error', None)
            if error_data:
                 st.error(f"Ø®Ø·Ø§ Ø¯Ø± Ø¯Ø±Ø¬ Ø¯Ø§Ø¯Ù‡ Ø¨Ù‡ Ø¬Ø¯ÙˆÙ„ {table_name}: {error_data.get('message', 'Ø®Ø·Ø§ÛŒ Ù†Ø§Ù…Ø´Ø®Øµ')}")
                 return False
            return True

    except Exception as e:
        st.error(f"Ø®Ø·Ø§ÛŒ Ú©Ù„ÛŒ Ø¯Ø± Ø¯Ø±Ø¬ Ø¯Ø§Ø¯Ù‡ Ø¨Ù‡ Ø¬Ø¯ÙˆÙ„ {table_name}: {e}")
        return False
        
def calculate_oee_metrics(df_prod, df_err):
    if df_prod.empty:
        return 0, 0, 0, 0, 0, 0, 0, 0 

    total_planned_time_min = df_prod["Duration"].sum() * 60
    total_down_time_min = df_err["Duration"].sum()
    operating_time_min = max(0, total_planned_time_min - total_down_time_min)

    availability_pct = (operating_time_min / total_planned_time_min) * 100 if total_planned_time_min > 0 else 0
    
    total_pack_qty = df_prod["PackQty"].sum()
    total_waste = df_prod["Waste"].sum()
    total_good_qty = total_pack_qty - total_waste

    quality_pct = (total_good_qty / total_pack_qty) * 100 if total_pack_qty > 0 else 0
    
    avg_capacity_units_per_hour = df_prod["Capacity"].mean() 
    ideal_cycle_rate_per_min = avg_capacity_units_per_hour / 60 if avg_capacity_units_per_hour > 0 else 0
        
    theoretical_run_time_min = total_pack_qty / ideal_cycle_rate_per_min if ideal_cycle_rate_per_min > 0 else 0
        
    performance_pct = (theoretical_run_time_min / operating_time_min) * 100 if operating_time_min > 0 else 0
    performance_pct = min(performance_pct, 100) 
        
    oee_pct = (availability_pct / 100) * (performance_pct / 100) * (quality_pct / 100) * 100
    
    total_potential_packages = total_planned_time_min * ideal_cycle_rate_per_min
    line_efficiency_pct = (total_good_qty / total_potential_packages) * 100 if total_potential_packages > 0 else 0
    line_efficiency_pct = min(line_efficiency_pct, 100)
    
    return oee_pct, line_efficiency_pct, availability_pct, performance_pct, quality_pct, total_down_time_min, total_good_qty, total_pack_qty


# ------------------------------------------------------------------------------
# --- Û³. Ù…Ù†Ø·Ù‚ Ø§ØµÙ„ÛŒ Ø¨Ø±Ù†Ø§Ù…Ù‡ Ùˆ Ù†Ø§ÙˆØ¨Ø±ÛŒ (Ø¨Ø§Ø²Ø³Ø§Ø²ÛŒ Ø´Ø¯Ù‡) ---
# ------------------------------------------------------------------------------

def process_and_insert_data(uploaded_files, sheet_name_to_process):
    """Ø¢Ù¾Ù„ÙˆØ¯ ÙØ§ÛŒÙ„â€ŒÙ‡Ø§ Ùˆ Ø¯Ø±Ø¬ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ Ø¨Ù‡ Ø¬Ø¯Ø§ÙˆÙ„."""
    total_files = len(uploaded_files)
    success_count = 0
    
    st.markdown("### Û±. Ø¢Ø±Ø´ÛŒÙˆ ÙØ§ÛŒÙ„â€ŒÙ‡Ø§ÛŒ Ø®Ø§Ù… (Storage)")
    upload_to_supabase(uploaded_files) 
    
    st.markdown("### Û². Ù¾Ø±Ø¯Ø§Ø²Ø´ Ùˆ Ø¯Ø±Ø¬ Ø¯Ø§Ø¯Ù‡ Ø¨Ù‡ Ø¬Ø¯Ø§ÙˆÙ„ (PostgreSQL)")
    status = st.status("Ø¯Ø± Ø­Ø§Ù„ Ù¾Ø±Ø¯Ø§Ø²Ø´ Ùˆ Ø¯Ø±Ø¬ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§...", expanded=True)
    
    for i, file in enumerate(uploaded_files):
        original_filename = file.name
        file_date_obj = parse_filename_date_to_datetime(original_filename)
        
        if not file_date_obj:
            status.write(f"âŒ ÙØ§ÛŒÙ„ **{original_filename}**: ØªØ§Ø±ÛŒØ® Ø¯Ø± Ù†Ø§Ù… ÙØ§ÛŒÙ„ Ù¾ÛŒØ¯Ø§ Ù†Ø´Ø¯. Skip.")
            continue
            
        status.write(f"Ø¯Ø± Ø­Ø§Ù„ Ù¾Ø±Ø¯Ø§Ø²Ø´ ÙØ§ÛŒÙ„: **{original_filename}** (ØªØ§Ø±ÛŒØ®: {file_date_obj})")
        
        try:
            df_raw_sheet = pd.read_excel(BytesIO(file.getvalue()), sheet_name=sheet_name_to_process, header=None)

            prod_df = read_production_data(df_raw_sheet, original_filename, sheet_name_to_process, file_date_obj)
            err_df = read_error_data(df_raw_sheet, sheet_name_to_process, original_filename, file_date_obj)

            if not prod_df.empty and 'PackQty' in prod_df.columns:
                prod_success = insert_to_db(prod_df, PROD_TABLE)
                if prod_success:
                    status.write(f"âœ… Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ ØªÙˆÙ„ÛŒØ¯ Ø¨Ø§ Ù…ÙˆÙÙ‚ÛŒØª Ø¨Ù‡ Ø¬Ø¯ÙˆÙ„ `{PROD_TABLE}` Ø¯Ø±Ø¬ Ø´Ø¯.")
                else:
                    status.write(f"âŒ Ø®Ø·Ø§ÛŒ Ø¯Ø±Ø¬ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ ØªÙˆÙ„ÛŒØ¯ Ø¨Ù‡ `{PROD_TABLE}`.")
                    continue 
            else:
                status.write(f"âš ï¸ ÙØ§ÛŒÙ„ **{original_filename}** Ø­Ø§ÙˆÛŒ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ ØªÙˆÙ„ÛŒØ¯ Ù…Ø¹ØªØ¨Ø± Ù†Ø¨ÙˆØ¯ ÛŒØ§ Ø®Ø§Ù„ÛŒ Ø¨ÙˆØ¯.")

            if not err_df.empty and 'Duration' in err_df.columns:
                err_success = insert_to_db(err_df, ERROR_TABLE)
                if err_success:
                    status.write(f"âœ… Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ Ø®Ø·Ø§ Ø¨Ø§ Ù…ÙˆÙÙ‚ÛŒØª Ø¨Ù‡ Ø¬Ø¯ÙˆÙ„ `{ERROR_TABLE}` Ø¯Ø±Ø¬ Ø´Ø¯.")
                else:
                    status.write(f"âŒ Ø®Ø·Ø§ÛŒ Ø¯Ø±Ø¬ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ Ø®Ø·Ø§ Ø¨Ù‡ `{ERROR_TABLE}`.")
            else:
                 status.write(f"âš ï¸ ÙØ§ÛŒÙ„ **{original_filename}** Ø­Ø§ÙˆÛŒ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ Ø®Ø·Ø§ Ù…Ø¹ØªØ¨Ø± Ù†Ø¨ÙˆØ¯ ÛŒØ§ Ø®Ø§Ù„ÛŒ Ø¨ÙˆØ¯.")

            success_count += 1

        except ValueError as e:
            if 'Worksheet named' in str(e) and 'not found' in str(e):
                status.write(f"âŒ Ø®Ø·Ø§ Ø¯Ø± Ù¾Ø±Ø¯Ø§Ø²Ø´ ÙØ§ÛŒÙ„ Ø§Ú©Ø³Ù„ **'{original_filename}'** (Ø´ÛŒØª: {sheet_name_to_process}): Ø´ÛŒØª Ø¨Ø§ Ù†Ø§Ù… ÙˆØ§Ø±Ø¯ Ø´Ø¯Ù‡ Ù¾ÛŒØ¯Ø§ Ù†Ø´Ø¯. **Ù„Ø·ÙØ§Ù‹ Ù†Ø§Ù… Ø´ÛŒØª Ø±Ø§ Ø¨Ø±Ø±Ø³ÛŒ Ú©Ù†ÛŒØ¯.**")
            else:
                status.write(f"âŒ Ø®Ø·Ø§ÛŒ Ù¾Ø±Ø¯Ø§Ø²Ø´ ÙØ§ÛŒÙ„ Ø§Ú©Ø³Ù„ **'{original_filename}'** (Ø´ÛŒØª: {sheet_name_to_process}): Ø³Ø§Ø®ØªØ§Ø± Ø´ÛŒØª Ø§Ú©Ø³Ù„ Ù…Ø·Ø§Ø¨Ù‚Øª Ù†Ø¯Ø§Ø±Ø¯. (Ø¬Ø²Ø¦ÛŒØ§Øª Ø®Ø·Ø§: {e})")

        except Exception as e:
            status.write(f"âŒ Ø®Ø·Ø§ÛŒ Ù†Ø§Ù…Ø´Ø®Øµ Ù‡Ù†Ú¯Ø§Ù… Ù¾Ø±Ø¯Ø§Ø²Ø´ ÙØ§ÛŒÙ„ **'{original_filename}'**: {e}")

    if success_count == total_files:
        status.update(label="âœ… ØªÙ…Ø§Ù… ÙØ§ÛŒÙ„â€ŒÙ‡Ø§ Ø¨Ø§ Ù…ÙˆÙÙ‚ÛŒØª Ù¾Ø±Ø¯Ø§Ø²Ø´ Ùˆ Ø¢Ù¾Ù„ÙˆØ¯ Ø´Ø¯Ù†Ø¯!", state="complete", expanded=False)
    else:
        status.update(label=f"âš ï¸ {success_count} Ø§Ø² {total_files} ÙØ§ÛŒÙ„ Ø¨Ø§ Ù…ÙˆÙÙ‚ÛŒØª Ù¾Ø±Ø¯Ø§Ø²Ø´ Ø´Ø¯Ù†Ø¯. Ø¬Ø²Ø¦ÛŒØ§Øª Ø±Ø§ Ø¨Ø±Ø±Ø³ÛŒ Ú©Ù†ÛŒØ¯.", state="error", expanded=True)
        
    st.cache_data.clear() 
    time.sleep(1) 
    st.rerun() 

# --- Ù†Ø§ÙˆØ¨Ø±ÛŒ Ø§ØµÙ„ÛŒ Ø¨Ø±Ù†Ø§Ù…Ù‡ (Ù…Ù†ÙˆÛŒ Ø¬Ø§Ù…Ø¹) ---

if 'page' not in st.session_state:
    st.session_state.page = "Dashboard & KPIs" 

st.sidebar.header("Ù…Ù†ÙˆÛŒ Ø¨Ø±Ù†Ø§Ù…Ù‡")
# Ù…Ù†ÙˆÛŒ Ú©Ø§Ù…Ù„ Ø´Ø§Ù…Ù„ ØªÙ…Ø§Ù…ÛŒ Ø¨Ø®Ø´â€ŒÙ‡Ø§ÛŒ ØªØ­Ù„ÛŒÙ„ Ùˆ Ù…Ø¯ÛŒØ±ÛŒØªÛŒ
page_options = ["ğŸ“Š Dashboard & KPIs", "ğŸ“ˆ Advanced Trend Analysis", "â¬†ï¸ Upload Data", "ğŸ—„ï¸ Data Archive", "ğŸ“§ Contact Me"] 
try:
    selected_page_index = page_options.index(st.session_state.page)
except ValueError:
    selected_page_index = 0 # Default to Dashboard if state is invalid

selected_page = st.sidebar.radio("Ø¨Ø±Ùˆ Ø¨Ù‡:", options=page_options, index=selected_page_index, key="sidebar_radio")

if selected_page != st.session_state.page:
    st.session_state.page = selected_page
    st.rerun()

# ------------------------------------------------------------------------------
# --- Û´. Ù¾ÛŒØ§Ø¯Ù‡â€ŒØ³Ø§Ø²ÛŒ ØµÙØ­Ø§Øª ---
# ------------------------------------------------------------------------------

# --- ØµÙØ­Ù‡ Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ Ø¯Ø§Ø¯Ù‡ ---
if st.session_state.page == "â¬†ï¸ Upload Data":
    st.header("â¬†ï¸ Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ ÙØ§ÛŒÙ„â€ŒÙ‡Ø§ÛŒ Ø§Ú©Ø³Ù„ Ùˆ Ø¯Ø±Ø¬ Ø¯Ø± Ù¾Ø§ÛŒÚ¯Ø§Ù‡ Ø¯Ø§Ø¯Ù‡")

    sheet_name_to_process = st.text_input(
        "Ù†Ø§Ù… Ø´ÛŒØª (Sheet Name) Ø­Ø§ÙˆÛŒ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§:",
        value="daily", 
        help="Ù†Ø§Ù… Ø¯Ù‚ÛŒÙ‚ Ø´ÛŒØªÛŒ Ú©Ù‡ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ ØªÙˆÙ„ÛŒØ¯ Ùˆ Ø®Ø·Ø§ Ø¯Ø± Ø¢Ù† Ù‚Ø±Ø§Ø± Ø¯Ø§Ø±Ù†Ø¯."
    )

    uploaded_files = st.file_uploader(
        "ÙØ§ÛŒÙ„â€ŒÙ‡Ø§ÛŒ Ø§Ú©Ø³Ù„ (.xlsx) Ø±Ø§ Ø§ÛŒÙ†Ø¬Ø§ Ø¢Ù¾Ù„ÙˆØ¯ Ú©Ù†ÛŒØ¯", 
        type=["xlsx"], 
        accept_multiple_files=True
    )

    if st.button("ğŸš€ Ø´Ø±ÙˆØ¹ Ù¾Ø±Ø¯Ø§Ø²Ø´ Ùˆ Ø¯Ø±Ø¬ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§"):
        if not uploaded_files:
            st.warning("Ù„Ø·ÙØ§Ù‹ Ø§Ø¨ØªØ¯Ø§ ÙØ§ÛŒÙ„â€ŒÙ‡Ø§ Ø±Ø§ Ø§Ù†ØªØ®Ø§Ø¨ Ú©Ù†ÛŒØ¯.")
        elif not sheet_name_to_process.strip():
            st.error("Ù„Ø·ÙØ§Ù‹ Ù†Ø§Ù… Ø´ÛŒØª (Sheet Name) Ø±Ø§ ÙˆØ§Ø±Ø¯ Ú©Ù†ÛŒØ¯.")
        else:
            process_and_insert_data(uploaded_files, sheet_name_to_process.strip())

# --- ØµÙØ­Ù‡ Ø¢Ø±Ø´ÛŒÙˆ Ø¯Ø§Ø¯Ù‡ ---
elif st.session_state.page == "ğŸ—„ï¸ Data Archive":
    st.header("ğŸ—„ï¸ Ù…Ø¯ÛŒØ±ÛŒØª Ùˆ Ø­Ø°Ù Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ Ø¢Ø±Ø´ÛŒÙˆ")
    
    st.error("âš ï¸ Ù‡Ø´Ø¯Ø§Ø±: Ø§ÛŒÙ† Ø¨Ø®Ø´ Ø¨Ù‡ Ø´Ù…Ø§ Ø§Ø¬Ø§Ø²Ù‡ Ù…ÛŒâ€ŒØ¯Ù‡Ø¯ ØªØ§ ØªÙ…Ø§Ù… Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ ÛŒÚ© Ø¬Ø¯ÙˆÙ„ Ø±Ø§ Ø­Ø°Ù Ú©Ù†ÛŒØ¯. Ø§ÛŒÙ† Ø¹Ù…Ù„ ØºÛŒØ±Ù‚Ø§Ø¨Ù„ Ø¨Ø§Ø²Ú¯Ø´Øª Ø§Ø³Øª.")

    table_to_delete = st.selectbox(
        "Ø¬Ø¯ÙˆÙ„ Ù…ÙˆØ±Ø¯ Ù†Ø¸Ø± Ø¨Ø±Ø§ÛŒ Ø­Ø°Ù Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§:",
        [PROD_TABLE, ERROR_TABLE],
        key="archive_table_select"
    )
    
    delete_password = st.text_input("Ø±Ù…Ø² Ø¹Ø¨ÙˆØ± Ø­Ø°Ù:", type="password")
    
    delete_button_clicked = st.button(
        f"ğŸ”¥ Ø­Ø°Ù ØªÙ…Ø§Ù… Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ Ø¬Ø¯ÙˆÙ„ '{table_to_delete}'", 
        type="primary", 
        use_container_width=True
    )
    
    if delete_button_clicked:
        if delete_password == ARCHIVE_DELETE_PASSWORD:
            try:
                supabase.table(table_to_delete).delete().neq('id', '0').execute() 
                st.cache_data.clear() 
                st.success(f"âœ… ØªÙ…Ø§Ù… Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ Ø¬Ø¯ÙˆÙ„ **{table_to_delete}** Ø¨Ø§ Ù…ÙˆÙÙ‚ÛŒØª Ø­Ø°Ù Ø´Ø¯Ù†Ø¯.")
                st.rerun()
            except Exception as e:
                st.error(f"âŒ Ø®Ø·Ø§ÛŒ Ø­Ø°Ù: {e}")
        else:
            st.error("Ø±Ù…Ø² Ø¹Ø¨ÙˆØ± Ø­Ø°Ù Ø§Ø´ØªØ¨Ø§Ù‡ Ø§Ø³Øª.")


# --- ØµÙØ­Ù‡ ØªÙ…Ø§Ø³ Ø¨Ø§ Ù…Ù† ---
elif st.session_state.page == "ğŸ“§ Contact Me":
    st.header("ğŸ“§ ØªÙ…Ø§Ø³ Ø¨Ø§ ØªÙˆØ³Ø¹Ù‡â€ŒØ¯Ù‡Ù†Ø¯Ù‡")
    st.markdown("---")
    st.markdown("""
    ### Ø¯Ø±Ø¨Ø§Ø±Ù‡ Ø§ÛŒÙ† Ù¾Ù„ØªÙØ±Ù… ğŸ’¡

    Ø¯Ø± Ø¯Ù†ÛŒØ§ÛŒ Ø§Ù…Ø±ÙˆØ² Ø¨Ø§ Ù¾ÛŒØ´Ø±ÙØªâ€ŒÙ‡Ø§ÛŒ Ø³Ø±ÛŒØ¹ ØªÚ©Ù†ÙˆÙ„ÙˆÚ˜ÛŒØŒ Ù‡ÙˆØ´ Ù…ØµÙ†ÙˆØ¹ÛŒ (AI) Ø¯ÛŒÚ¯Ø± ÛŒÚ© Ø§Ù†ØªØ®Ø§Ø¨ Ù†ÛŒØ³ØªØŒ Ø¨Ù„Ú©Ù‡ ÛŒÚ© **Ø¶Ø±ÙˆØ±Øª** Ø§Ø³Øª. Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² Ù‡ÙˆØ´ Ù…ØµÙ†ÙˆØ¹ÛŒ Ù…ÛŒâ€ŒØªÙˆØ§Ù†Ø¯ Ø¨Ù‡ Ø·ÙˆØ± Ú†Ø´Ù…Ú¯ÛŒØ±ÛŒ Ø¹Ù…Ù„Ú©Ø±Ø¯ Ø±Ø§ Ø§ÙØ²Ø§ÛŒØ´ Ø¯Ù‡Ø¯ØŒ Ø®Ø·Ø§ÛŒ Ø§Ù†Ø³Ø§Ù†ÛŒ Ø±Ø§ Ø¨Ù‡ Ø­Ø¯Ø§Ù‚Ù„ Ø¨Ø±Ø³Ø§Ù†Ø¯ Ùˆ Ø¬Ø±ÛŒØ§Ù† Ú©Ø§Ø±ÛŒ Ø±Ø§ Ø³Ø§Ø¯Ù‡ Ú©Ù†Ø¯. ØªÚ©ÛŒÙ‡ ØµØ±Ù Ø¨Ù‡ Ø±ÙˆØ´â€ŒÙ‡Ø§ÛŒ Ø³Ù†ØªÛŒ Ø§ØºÙ„Ø¨ Ù…Ù†Ø¬Ø± Ø¨Ù‡ Ù‡Ø¯Ø± Ø±ÙØªÙ† Ø²Ù…Ø§Ù† Ùˆ ØªÙ„Ø§Ø´ Ù…ÛŒâ€ŒØ´ÙˆØ¯ØŒ Ø¨Ø¯ÙˆÙ† Ø¢Ù†Ú©Ù‡ Ú©Ø§Ø±Ø§ÛŒÛŒ Ù„Ø§Ø²Ù… Ø­Ø§ØµÙ„ Ø´ÙˆØ¯.

    Ø¨Ø±Ø§ÛŒ Ù¾Ø§Ø³Ø® Ø¨Ù‡ Ø§ÛŒÙ† Ù†ÛŒØ§Ø²ØŒ Ù…Ù† ØªÙˆØ³Ø¹Ù‡ Ù¾Ù„ØªÙØ±Ù…ÛŒ Ø±Ø§ Ø¢ØºØ§Ø² Ú©Ø±Ø¯Ù… Ú©Ù‡ **Ø§ØªÙˆÙ…Ø§Ø³ÛŒÙˆÙ†** Ø±Ø§ Ø¨Ø§ **Ù‡ÙˆØ´Ù…Ù†Ø¯ÛŒ** ØªØ±Ú©ÛŒØ¨ Ù…ÛŒâ€ŒÚ©Ù†Ø¯. Ø¨Ø§ Ø§Ù„Ù‡Ø§Ù… Ø§Ø² Ø¹Ù„Ø§Ù‚Ù‡â€ŒØ§Ù… Ø¨Ù‡ Ù¾Ø§ÛŒØªÙˆÙ† (Ø¨Ø§ ÙˆØ¬ÙˆØ¯ Ø§ÛŒÙ†Ú©Ù‡ Ù‡Ù†ÙˆØ² Ø¯Ø± Ø­Ø§Ù„ ÛŒØ§Ø¯Ú¯ÛŒØ±ÛŒ Ù‡Ø³ØªÙ…) Ùˆ Ø§Ø´ØªÛŒØ§Ù‚ Ø¹Ù…ÛŒÙ‚ Ø¨Ø±Ø§ÛŒ Ø§ÛŒØ¬Ø§Ø¯ Ø±Ø§Ù‡â€ŒØ­Ù„â€ŒÙ‡Ø§ÛŒ ÙÙ†ÛŒ Ù…Ù†Ø¸Ù… Ùˆ Ù…Ø¨ØªÙ†ÛŒ Ø¨Ø± Ø¯Ø§Ø¯Ù‡ØŒ ØªÙˆØ³Ø¹Ù‡ Ø§ÛŒÙ† ÙˆØ¨â€ŒØ³Ø§ÛŒØª Ù…Ø¨ØªÙ†ÛŒ Ø¨Ø± Streamlit Ø±Ø§ Ø¨Ø±Ø§ÛŒ ØªØ­Ù„ÛŒÙ„ Ø±ÙˆØ²Ø§Ù†Ù‡ Ø¹Ù…Ù„Ú©Ø±Ø¯ ØªÙˆÙ„ÛŒØ¯ Ø¢ØºØ§Ø² Ú©Ø±Ø¯Ù….

    Ø¯Ø± Ø·ÙˆÙ„ Ø§ÛŒÙ† ÙØ±Ø¢ÛŒÙ†Ø¯ØŒ Ø§Ø¨Ø²Ø§Ø±Ù‡Ø§ÛŒÛŒ Ù…Ø§Ù†Ù†Ø¯ **Gemini AI** Ø¯Ø± Ø§Ø´Ú©Ø§Ù„â€ŒØ²Ø¯Ø§ÛŒÛŒØŒ Ù¾Ø§Ù„Ø§ÛŒØ´ Ø§Ø³ØªØ±Ø§ØªÚ˜ÛŒâ€ŒÙ‡Ø§ Ùˆ Ø¨Ù‡ Ø­Ù‚ÛŒÙ‚Øª Ù¾ÛŒÙˆØ³ØªÙ† Ø§ÛŒÙ† Ø§ÛŒØ¯Ù‡ Ø¨Ø³ÛŒØ§Ø± Ù…Ø¤Ø«Ø± Ø¨ÙˆØ¯Ù†Ø¯. ØµØ§Ø¯Ù‚Ø§Ù†Ù‡ Ø¨Ú¯ÙˆÛŒÙ…ØŒ Ø¨Ø¯ÙˆÙ† Ú©Ù…Ú© Ù‡ÙˆØ´ Ù…ØµÙ†ÙˆØ¹ÛŒØŒ Ø±Ø³ÛŒØ¯Ù† Ø¨Ù‡ Ø§ÛŒÙ† Ù†Ù‚Ø·Ù‡ Ø¨Ø³ÛŒØ§Ø± Ø¯Ø´ÙˆØ§Ø±ØªØ± Ù…ÛŒâ€ŒØ´Ø¯.

    Ù…Ù† Ù…ØªØ¹Ù‡Ø¯ Ø¨Ù‡ Ø¨Ù‡Ø¨ÙˆØ¯ Ù‡Ø³ØªÙ…â€”Ù‡Ù… Ø¯Ø± Ú©Ø¯Ù†ÙˆÛŒØ³ÛŒ Ùˆ Ù‡Ù… Ø¯Ø± Ø·Ø±Ø§Ø­ÛŒ Ø³ÛŒØ³ØªÙ…. Ø§Ø² Ø¨Ø§Ø²Ø®ÙˆØ±Ø¯ØŒ Ù¾ÛŒØ´Ù†Ù‡Ø§Ø¯Ø§ØªØŒ ÛŒØ§ Ù‡Ø± Ú¯ÙˆÙ†Ù‡ Ø±Ø§Ù‡Ù†Ù…Ø§ÛŒÛŒ Ø¨Ø±Ø§ÛŒ Ø¨Ù‡Ø¨ÙˆØ¯ Ø¨ÛŒØ´ØªØ± Ø§ÛŒÙ† Ù¾Ù„ØªÙØ±Ù… Ø§Ø³ØªÙ‚Ø¨Ø§Ù„ Ù…ÛŒâ€ŒÚ©Ù†Ù….

    ---

    **Ø§Ø·Ù„Ø§Ø¹Ø§Øª ØªÙ…Ø§Ø³:**

    ğŸ“§ **Ø§ÛŒÙ…ÛŒÙ„:** m.asdz@yahoo.com  
    ğŸ”— **Ù„ÛŒÙ†Ú©Ø¯ÛŒÙ†:** [Mohammad Asdollahzadeh](https://www.linkedin.com/in/mohammad-asdollahzadeh)

    Ø¨Ø§ ØªØ´Ú©Ø± Ø§Ø² Ø¨Ø§Ø²Ø¯ÛŒØ¯ Ø´Ù…Ø§ØŒ Ùˆ Ø§Ø² Ø­Ù…Ø§ÛŒØª Ø´Ù…Ø§ ØµÙ…ÛŒÙ…Ø§Ù†Ù‡ Ù‚Ø¯Ø±Ø¯Ø§Ù†ÛŒ Ù…ÛŒâ€ŒÚ©Ù†Ù….

    Ø¨Ø§ Ø§Ø­ØªØ±Ø§Ù…ØŒ
    **Ù…Ø­Ù…Ø¯ Ø§Ø³Ø¯Ø§Ù„Ù„Ù‡ Ø²Ø§Ø¯Ù‡**
    """)


# --- ØµÙØ­Ù‡ ØªØ­Ù„ÛŒÙ„ Ø±ÙˆÙ†Ø¯ (Ù¾ÛŒØ´Ø±ÙØªÙ‡) ---
elif st.session_state.page == "ğŸ“ˆ Advanced Trend Analysis":
    st.header("ğŸ“ˆ ØªØ­Ù„ÛŒÙ„ Ø±ÙˆÙ†Ø¯ Ù¾ÛŒØ´Ø±ÙØªÙ‡ (Trend Analysis)")

    df_prod_all = load_data_from_supabase_tables(PROD_TABLE)
    df_err_all = load_data_from_supabase_tables(ERROR_TABLE)

    if df_prod_all.empty:
        st.warning("Ø¯Ø§Ø¯Ù‡â€ŒØ§ÛŒ Ø¨Ø±Ø§ÛŒ ØªØ­Ù„ÛŒÙ„ Ø±ÙˆÙ†Ø¯ ÙˆØ¬ÙˆØ¯ Ù†Ø¯Ø§Ø±Ø¯. Ù„Ø·ÙØ§Ù‹ Ø§Ø¨ØªØ¯Ø§ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ Ø±Ø§ Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ Ú©Ù†ÛŒØ¯.")
    else:
        # Aggregate data by Date for Trend Analysis (Comprehensive)
        daily_summary = df_prod_all.groupby("Date").agg(
            TotalPackQty=('PackQty', 'sum'),
            TotalWaste=('Waste', 'sum'),
            TotalDuration=('Duration', 'sum'), # in hours
            AvgCapacity=('Capacity', 'mean')
        ).reset_index()
        
        daily_errors = df_err_all.groupby("Date")["Duration"].sum().reset_index().rename(columns={"Duration": "TotalDowntime"}) # in minutes
        daily_df = pd.merge(daily_summary, daily_errors, on="Date", how="left").fillna(0)
        
        # OEE/KPI Calculation on Daily Data
        daily_df['TotalDurationMin'] = daily_df['TotalDuration'] * 60
        daily_df['OperatingTime'] = daily_df['TotalDurationMin'] - daily_df['TotalDowntime']
        daily_df['OperatingTime'] = daily_df['OperatingTime'].apply(lambda x: max(0, x))
        
        daily_df['Availability'] = np.where(daily_df['TotalDurationMin'] > 0, (daily_df['OperatingTime'] / daily_df['TotalDurationMin']) * 100, 0)
        daily_df['TotalGoodQty'] = daily_df['TotalPackQty'] - daily_df['TotalWaste']
        daily_df['Quality'] = np.where(daily_df['TotalPackQty'] > 0, (daily_df['TotalGoodQty'] / daily_df['TotalPackQty']) * 100, 0)
        daily_df['IdealCycleRatePerMin'] = daily_df['AvgCapacity'] / 60
        daily_df['TheoreticalRunTime'] = np.where(daily_df['IdealCycleRatePerMin'] > 0, daily_df['TotalPackQty'] / daily_df['IdealCycleRatePerMin'], 0)
        daily_df['Performance'] = np.where(daily_df['OperatingTime'] > 0, (daily_df['TheoreticalRunTime'] / daily_df['OperatingTime']) * 100, 0)
        daily_df['Performance'] = daily_df['Performance'].apply(lambda x: min(x, 100))
        daily_df['OEE'] = (daily_df['Availability'] / 100) * (daily_df['Performance'] / 100) * (daily_df['Quality'] / 100) * 100
        
        # --- Display Trend Charts ---
        st.subheader("Ø±ÙˆÙ†Ø¯ Ú©Ù„ÛŒ OEE Ùˆ Ø§Ø¬Ø²Ø§ÛŒ Ø¢Ù†")
        fig_trend = px.line(daily_df, x="Date", y=["OEE", "Availability", "Performance", "Quality"], 
                            title="Ø±ÙˆÙ†Ø¯ Ø±ÙˆØ²Ø§Ù†Ù‡ OEEØŒ Ø¯Ø³ØªØ±Ø³ÛŒØŒ Ø¹Ù…Ù„Ú©Ø±Ø¯ Ùˆ Ú©ÛŒÙÛŒØª",
                            labels={"value": "Ø¯Ø±ØµØ¯ (%)", "Date": "ØªØ§Ø±ÛŒØ®"},
                            template="plotly_dark") # Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² ØªÙ… ØªÛŒØ±Ù‡ Ø¨Ø±Ø§ÛŒ Ø¸Ø§Ù‡Ø± Ø­Ø±ÙÙ‡â€ŒØ§ÛŒâ€ŒØªØ±
        fig_trend.update_layout(legend_title_text='Ø´Ø§Ø®Øµ', height=500)
        st.plotly_chart(fig_trend, use_container_width=True)
        
        st.markdown("---")
        
        st.subheader("Ø±ÙˆÙ†Ø¯ Ø±ÙˆØ²Ø§Ù†Ù‡ ØªÙˆÙ„ÛŒØ¯ (Ø¨Ø³ØªÙ‡) Ùˆ ØªÙˆÙ‚Ù (Downtime)")
        
        fig_dual = go.Figure()

        fig_dual.add_trace(go.Bar(
            x=daily_df['Date'],
            y=daily_df['TotalGoodQty'],
            name='ØªÙˆÙ„ÛŒØ¯ Ø®Ø§Ù„Øµ (Ø¨Ø³ØªÙ‡)',
            yaxis='y1',
            marker_color='skyblue'
        ))

        fig_dual.add_trace(go.Scatter(
            x=daily_df['Date'],
            y=daily_df['TotalDowntime'],
            name='ØªÙˆÙ‚Ù Ú©Ù„ (Ø¯Ù‚ÛŒÙ‚Ù‡)',
            yaxis='y2',
            mode='lines+markers',
            marker_color='red'
        ))

        fig_dual.update_layout(
            title='ØªØ­Ù„ÛŒÙ„ Ø¯ÙˆÚ¯Ø§Ù†Ù‡ Ø±ÙˆÙ†Ø¯ ØªÙˆÙ„ÛŒØ¯ Ùˆ ØªÙˆÙ‚Ù',
            template="plotly_dark",
            yaxis=dict(
                title='ØªÙˆÙ„ÛŒØ¯ Ø®Ø§Ù„Øµ (Ø¨Ø³ØªÙ‡)',
                titlefont=dict(color='skyblue'),
                tickfont=dict(color='skyblue')
            ),
            yaxis2=dict(
                title='ØªÙˆÙ‚Ù Ú©Ù„ (Ø¯Ù‚ÛŒÙ‚Ù‡)',
                titlefont=dict(color='red'),
                tickfont=dict(color='red'),
                overlaying='y',
                side='right'
            ),
            legend=dict(x=0, y=1.1, orientation="h")
        )
        st.plotly_chart(fig_dual, use_container_width=True)


# --- ØµÙØ­Ù‡ Ø¯Ø§Ø´Ø¨ÙˆØ±Ø¯ Ùˆ Ø´Ø§Ø®Øµâ€ŒÙ‡Ø§ÛŒ Ú©Ù„ÛŒØ¯ÛŒ (Ø¬Ø§Ù…Ø¹) ---
elif st.session_state.page == "ğŸ“Š Dashboard & KPIs":
    st.header("ğŸ“Š Ø¯Ø§Ø´Ø¨ÙˆØ±Ø¯ ØªØ­Ù„ÛŒÙ„ Ø¬Ø§Ù…Ø¹ Ø¹Ù…Ù„Ú©Ø±Ø¯")
    
    # --- Connection Status Check ---
    try:
        prod_count_response = supabase.table(PROD_TABLE).select("*", count='exact').limit(0).execute() 
        err_count_response = supabase.table(ERROR_TABLE).select("*", count='exact').limit(0).execute() 
        
        prod_count = prod_count_response.count
        err_count = err_count_response.count
        
        st.success(f"âœ… Ø§ØªØµØ§Ù„ Ø¨Ù‡ Supabase Ø¨Ø±Ù‚Ø±Ø§Ø± Ø§Ø³Øª. (Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ ØªÙˆÙ„ÛŒØ¯: {prod_count} Ø³Ø·Ø±ØŒ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ Ø®Ø·Ø§: {err_count} Ø³Ø·Ø±)")
    except Exception as e:
        st.error(f"âŒ Ø®Ø·Ø§ÛŒ Ø­ÛŒØ§ØªÛŒ: Ø§ØªØµØ§Ù„ Ø¨Ù‡ Supabase Ù‚Ø·Ø¹ Ø§Ø³Øª. Ù„Ø·ÙØ§Ù‹ ÙˆØ¶Ø¹ÛŒØª API Key Ùˆ Ø¬Ø¯Ø§ÙˆÙ„ Ø±Ø§ Ø¨Ø±Ø±Ø³ÛŒ Ú©Ù†ÛŒØ¯. (Ø¬Ø²Ø¦ÛŒØ§Øª Ø®Ø·Ø§: {e})")
        st.stop()
    st.markdown("---")
    
    df_prod_all = load_data_from_supabase_tables(PROD_TABLE)
    df_err_all = load_data_from_supabase_tables(ERROR_TABLE)

    if df_prod_all.empty:
        st.warning("Ø¯Ø§Ø¯Ù‡â€ŒØ§ÛŒ Ø¨Ø±Ø§ÛŒ ØªØ­Ù„ÛŒÙ„ ÙˆØ¬ÙˆØ¯ Ù†Ø¯Ø§Ø±Ø¯. Ù„Ø·ÙØ§Ù‹ Ø§Ø¨ØªØ¯Ø§ Ø§Ø² Ø¨Ø®Ø´ 'Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ ÙØ§ÛŒÙ„â€ŒÙ‡Ø§'ØŒ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ Ø±Ø§ Ø¯Ø±Ø¬ Ú©Ù†ÛŒØ¯.")
        st.markdown("---")
    else:
        # --- Filters ---
        col_filters, col_date = st.columns([1, 3])
        
        min_available_date = df_prod_all['Date'].min()
        max_available_date = df_prod_all['Date'].max()
        
        with col_date:
            date_range = st.date_input(
                "Ø¨Ø§Ø²Ù‡ ØªØ§Ø±ÛŒØ®:",
                value=(min_available_date, max_available_date),
                min_value=min_available_date,
                max_value=max_available_date,
                key="dashboard_date_range"
            )
            if len(date_range) == 2:
                selected_start_date, selected_end_date = date_range
            else:
                selected_start_date, selected_end_date = min_available_date, max_available_date
        
        df_prod_filtered = df_prod_all[
            (df_prod_all['Date'] >= selected_start_date) & 
            (df_prod_all['Date'] <= selected_end_date)
        ].copy()
        df_err_filtered = df_err_all[
            (df_err_all['Date'] >= selected_start_date) & 
            (df_err_all['Date'] <= selected_end_date)
        ].copy()

        unique_machines = ['Total Production'] + sorted(df_prod_filtered["ProductTypeForTon"].unique().tolist())
        with col_filters:
            selected_machine = st.selectbox("Ø§Ù†ØªØ®Ø§Ø¨ Ø®Ø· ØªÙˆÙ„ÛŒØ¯:", unique_machines)

        if selected_machine != 'Total Production':
            df_prod_filtered = df_prod_filtered[
                df_prod_filtered["ProductTypeForTon"] == selected_machine
            ].copy()
            # ÙÛŒÙ„ØªØ± Ú©Ø±Ø¯Ù† Ø®Ø·Ø§Ù‡Ø§ÛŒ Ù…Ø±Ø¨ÙˆØ· Ø¨Ù‡ Ù…Ø§Ø´ÛŒÙ† Ø§Ù†ØªØ®Ø§Ø¨ Ø´Ø¯Ù‡ (ÙØ±Ø¶ Ø¨Ø± Ø§ÛŒÙ† Ø§Ø³Øª Ú©Ù‡ Ù†Ø§Ù… Ù…Ø§Ø´ÛŒÙ† Ø¯Ø± Error Data Ø¨Ù‡ Ø­Ø±ÙˆÙ Ú©ÙˆÚ†Ú© Ø°Ø®ÛŒØ±Ù‡ Ù…ÛŒâ€ŒØ´ÙˆØ¯)
            df_err_filtered = df_err_filtered[
                df_err_filtered["machinetype"].str.contains(selected_machine.lower(), case=False, na=False)
            ].copy()


        # --- OEE Calculations ---
        oee_pct, line_efficiency_pct, availability_pct, performance_pct, quality_pct, \
            total_down_time_min, total_good_qty, total_pack_qty = calculate_oee_metrics(df_prod_filtered, df_err_filtered)

        # ----------------------------------------------------------------------------------
        # --- Ù†Ù…Ø§ÛŒØ´ KPIs (ÙÙˆÙ‚ Ú¯Ø±Ø§ÙÛŒÚ©ÛŒ Ùˆ Ø­Ø±ÙÙ‡â€ŒØ§ÛŒ) ---
        # ----------------------------------------------------------------------------------
        st.markdown("### Ø´Ø§Ø®Øµâ€ŒÙ‡Ø§ÛŒ Ú©Ù„ÛŒØ¯ÛŒ Ø¹Ù…Ù„Ú©Ø±Ø¯ (KPIs)")
        col1, col2, col3, col4, col5 = st.columns(5)
        
        def display_metric_pro(col, label, value, color_threshold=85):
            # Ø¸Ø§Ù‡Ø± Ø­Ø±ÙÙ‡â€ŒØ§ÛŒâ€ŒØªØ± Ø¨Ø§ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² Ø±Ù†Ú¯â€ŒÙ‡Ø§ÛŒ ØªÛŒØ±Ù‡ Ùˆ Ø®Ø·ÙˆØ· Ø¨Ø§Ø±ÛŒÚ©
            color = '#2ECC71' if value >= color_threshold else ('#FFC300' if value >= (color_threshold-15) else '#FF4B4B')
            col.markdown(f"""
            <div style='
                border-left: 5px solid {color}; 
                border-radius: 4px; 
                padding: 10px; 
                text-align: right; 
                background-color: #1E1E1E;
                box-shadow: 2px 2px 5px rgba(0,0,0,0.2);
            '>
                <p style='font-size: 13px; margin-bottom: 5px; color: #aaa; text-align: left;'>{label}</p>
                <h2 style='margin-top: 0; color: {color}; font-size: 28px; text-align: left;'>{value:,.1f} %</h2>
            </div>
            """, unsafe_allow_html=True)

        # Ù†Ù…Ø§ÛŒØ´ OEE Ùˆ Ø§Ø¬Ø²Ø§ÛŒ Ø¢Ù†
        display_metric_pro(col1, "OEE (Ø§Ø«Ø±Ø¨Ø®Ø´ÛŒ Ú©Ù„ÛŒ)", oee_pct, color_threshold=75)
        display_metric_pro(col2, "Availability (Ø¯Ø³ØªØ±Ø³ÛŒ)", availability_pct, color_threshold=85)
        display_metric_pro(col3, "Performance (Ø¹Ù…Ù„Ú©Ø±Ø¯)", performance_pct, color_threshold=85)
        display_metric_pro(col4, "Quality (Ú©ÛŒÙÛŒØª)", quality_pct, color_threshold=95)
        display_metric_pro(col5, "Line Efficiency (Ø±Ø§Ù†Ø¯Ù…Ø§Ù† Ø®Ø·)", line_efficiency_pct, color_threshold=70)
        
        st.markdown("<br>", unsafe_allow_html=True)

        col_prod, col_downtime = st.columns(2)
        
        with col_prod:
            st.metric("ØªÙˆÙ„ÛŒØ¯ Ø®Ø§Ù„Øµ (Ø¨Ø³ØªÙ‡)", f"{total_good_qty:,.0f} Ø¨Ø³ØªÙ‡")
        with col_downtime:
            st.metric("ØªÙˆÙ‚Ù Ú©Ù„ (Downtime)", f"{total_down_time_min:,.0f} Ø¯Ù‚ÛŒÙ‚Ù‡")
        
        st.markdown("---")
        
        # ----------------------------------------------------------------------------------
        # --- Ø¨Ø®Ø´ Ù‡ÙˆØ´Ù…Ù†Ø¯: Ø¨ÛŒÙ†Ø´â€ŒÙ‡Ø§ÛŒ Ú©Ù„ÛŒØ¯ÛŒ (Intelligent Insights) ---
        # ----------------------------------------------------------------------------------
        st.subheader("ğŸ’¡ Ø¨ÛŒÙ†Ø´â€ŒÙ‡Ø§ÛŒ Ù‡ÙˆØ´Ù…Ù†Ø¯ (Key Insights)")
        
        if total_down_time_min > 0 and availability_pct < 85:
            st.warning(f"Ø¯Ø³ØªØ±Ø³ÛŒ Ø®Ø· (Availability) ØªÙ†Ù‡Ø§ **{availability_pct:.1f}%** Ø§Ø³Øª. **{total_down_time_min:,.0f} Ø¯Ù‚ÛŒÙ‚Ù‡** ØªÙˆÙ‚Ù Ø«Ø¨Øª Ø´Ø¯Ù‡ Ø§Ø³Øª. ØªÙ…Ø±Ú©Ø² Ø¨Ø± Ú©Ø§Ù‡Ø´ ØªÙˆÙ‚Ù Ø¶Ø±ÙˆØ±ÛŒ Ø§Ø³Øª.")
        elif performance_pct < 85:
            st.info(f"Ø¹Ù…Ù„Ú©Ø±Ø¯ (Performance) Ø®Ø· **{performance_pct:.1f}%** Ø§Ø³Øª. Ø¨Ø±Ø±Ø³ÛŒ Ø³Ø±Ø¹Øªâ€ŒÙ‡Ø§ÛŒ ØªÙˆÙ„ÛŒØ¯ (Capacity) Ùˆ Ø²Ù…Ø§Ù†â€ŒÙ‡Ø§ÛŒ Ø³ÛŒÚ©Ù„ Ø¨Ø±Ø§ÛŒ Ø¨Ù‡Ø¨ÙˆØ¯ Ú©Ø§Ø±Ø§ÛŒÛŒ Ø§Ù„Ø²Ø§Ù…ÛŒ Ø§Ø³Øª.")
        else:
            st.success("Ø¹Ù…Ù„Ú©Ø±Ø¯ Ú©Ù„ÛŒ Ø®Ø· Ø¯Ø± ÙˆØ¶Ø¹ÛŒØª Ø®ÙˆØ¨ÛŒ Ù‚Ø±Ø§Ø± Ø¯Ø§Ø±Ø¯. Ø¨Ø±Ø§ÛŒ Ø­ÙØ¸ Ø§ÛŒÙ† Ø±ÙˆÙ†Ø¯ØŒ Ú©ÛŒÙÛŒØª (Quality) Ø±Ø§ Ø²ÛŒØ± Ù†Ø¸Ø± Ø¯Ø§Ø´ØªÙ‡ Ø¨Ø§Ø´ÛŒØ¯.")

        st.markdown("---")

        # ----------------------------------------------------------------------------------
        # --- Ù†Ù…ÙˆØ¯Ø§Ø±Ù‡Ø§ÛŒ ØªØ­Ù„ÛŒÙ„ Ø¬Ø§Ù…Ø¹ (Everything possible) ---
        # ----------------------------------------------------------------------------------
        
        col_charts1, col_charts2 = st.columns(2)
        
        with col_charts1:
            if not df_err_filtered.empty:
                st.subheader("Û±Û° Ù…ÙˆØ±Ø¯ Ø¨Ø±ØªØ± Ø¯Ù„Ø§ÛŒÙ„ ØªÙˆÙ‚Ù (Parity of Loss)")
                
                # Ú¯Ø±ÙˆÙ‡â€ŒØ¨Ù†Ø¯ÛŒ Ø¨Ø± Ø§Ø³Ø§Ø³ Ø®Ø·Ø§ Ùˆ ÙÛŒÙ„ØªØ± Ú©Ø±Ø¯Ù†
                top_errors = df_err_filtered.groupby("error")["duration"].sum().reset_index()
                top_errors = top_errors.sort_values(by="duration", ascending=False).head(10)

                # Ù†Ù…ÙˆØ¯Ø§Ø± Bar Ø¨Ø±Ø§ÛŒ Downtime (ÙÙˆÙ‚ Ú¯Ø±Ø§ÙÛŒÚ©ÛŒ)
                fig_err = px.bar(top_errors, x="error", y="duration",
                                 title="ØªØ­Ù„ÛŒÙ„ Ù¾Ø§Ø±ÙØªÙˆ: Ø¯Ù„Ø§ÛŒÙ„ ØªÙˆÙ‚Ù (Ø¨Ø± Ø­Ø³Ø¨ Ø¯Ù‚ÛŒÙ‚Ù‡)",
                                 labels={"duration": "Ù…Ø¯Øª Ø²Ù…Ø§Ù† (Ø¯Ù‚ÛŒÙ‚Ù‡)", "error": "Ø¯Ù„ÛŒÙ„ ØªÙˆÙ‚Ù"},
                                 color="duration",
                                 color_continuous_scale=px.colors.sequential.Plotly3,
                                 template="plotly_dark")
                fig_err.update_traces(texttemplate='%{y:.1f}', textposition='outside')
                st.plotly_chart(fig_err, use_container_width=True)
            else:
                st.warning("Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ Ø®Ø·Ø§ Ø¨Ø±Ø§ÛŒ Ù†Ù…Ø§ÛŒØ´ Ø¯Ù„Ø§ÛŒÙ„ ØªÙˆÙ‚Ù ÛŒØ§ÙØª Ù†Ø´Ø¯.")

        with col_charts2:
            st.subheader("ØªÙˆØ²ÛŒØ¹ ØªÙˆÙ„ÛŒØ¯ (Ton) Ø¨Ø± Ø§Ø³Ø§Ø³ Ù…Ø­ØµÙˆÙ„")
            total_ton_per_product = df_prod_filtered.groupby("producttypeforton")["ton"].sum().reset_index()
            total_ton_per_product = total_ton_per_product.sort_values(by="ton", ascending=False)
            
            # Ù†Ù…ÙˆØ¯Ø§Ø± Treemap Ø¨Ø±Ø§ÛŒ Ø³Ù‡Ù… ØªÙˆÙ„ÛŒØ¯ (ÙÙˆÙ‚ Ú¯Ø±Ø§ÙÛŒÚ©ÛŒ)
            fig_ton = px.treemap(total_ton_per_product, path=[px.Constant("Total Production"), 'producttypeforton'], values="ton", 
                                 title="Ø³Ù‡Ù… Ù‡Ø± Ù…Ø­ØµÙˆÙ„ Ø¯Ø± ØªÙˆÙ„ÛŒØ¯ Ú©Ù„ (Ø¨Ø± Ø­Ø³Ø¨ ØªÙ†Ø§Ú˜)",
                                 color="ton", 
                                 color_continuous_scale=px.colors.sequential.Teal,
                                 template="plotly_dark")
            fig_ton.update_layout(margin=dict(t=50, l=25, r=25, b=25))
            st.plotly_chart(fig_ton, use_container_width=True)
